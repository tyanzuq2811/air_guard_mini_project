{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020312e4",
   "metadata": {},
   "source": [
    "# Th√≠ nghi·ªám 4: Hybrid œÑ (Tau) Schedules for Self-Training\n",
    "\n",
    "## üéØ M·ª•c ti√™u (OPTIMIZED)\n",
    "\n",
    "Test **2 adaptive œÑ schedules** trong Self-Training ƒë·ªÉ so s√°nh v·ªõi constant œÑ:\n",
    "- **Fixed 0.90**: Constant threshold (baseline)\n",
    "- **Aggressive**: Fast decay t·ª´ 0.95 ‚Üí 0.80 (extreme adaptive)\n",
    "\n",
    "**‚ö° Time Optimization**: Gi·∫£m t·ª´ 4 ‚Üí 2 schedules (saves ~25-30 minutes)\n",
    "- B·ªè Conservative (between Fixed/Aggressive, √≠t contrast)\n",
    "- B·ªè Linear Decay (t∆∞∆°ng t·ª± Aggressive, ch·ªâ m∆∞·ª£t h∆°n)\n",
    "\n",
    "## Adaptive œÑ Strategy\n",
    "\n",
    "### L√Ω do c·∫ßn Adaptive œÑ:\n",
    "- **Early iterations**: œÑ cao (0.95) ‚Üí ch·ªçn samples r·∫•t confident ‚Üí tr√°nh confirmation bias\n",
    "- **Later iterations**: œÑ th·∫•p (0.85-0.80) ‚Üí t·∫≠n d·ª•ng unlabeled data ‚Üí scale l√™n nhanh\n",
    "\n",
    "### ‚úÖ Fixed 0.90 (Baseline)\n",
    "- Constant œÑ = 0.90 su·ªët 10 iterations\n",
    "- Standard approach trong self-training\n",
    "- Stable nh∆∞ng c√≥ th·ªÉ b·ªè l·ª° pseudo-labels ch·∫•t l∆∞·ª£ng kh·∫£ d·ª•ng\n",
    "\n",
    "### ‚úÖ Aggressive (Extreme Adaptive)\n",
    "- Iteration 1: œÑ = 0.95 (very strict)\n",
    "- Iterations 2-3: œÑ = 0.90 (moderate)\n",
    "- Iterations 4-5: œÑ = 0.85 (relaxed)\n",
    "- Iterations 6-10: œÑ = 0.80 (most relaxed)\n",
    "- **Fast decay** ‚Üí maximize unlabeled data usage ‚Üí test accuracy/F1 c√≥ scale nhanh h∆°n?\n",
    "\n",
    "## Metrics ƒë√°nh gi√°:\n",
    "- Test Accuracy, Test F1-macro (final performance)\n",
    "- Validation Accuracy/F1 curves (learning trajectory)\n",
    "- Pseudo-labeling activity per iteration\n",
    "- Total pseudo-labels added\n",
    "- Correlation gi·ªØa œÑ v√† performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d6949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "SEMI_DATASET_PATH = \"data/processed/dataset_for_semi.parquet\"\n",
    "CUTOFF = \"2017-01-01\"\n",
    "\n",
    "# œÑ schedules to compare (OPTIMIZED: 2 extremes for clearest contrast)\n",
    "TAU_SCHEDULES = {\n",
    "    \"Fixed_0.90\": [0.90] * 10,  # Baseline: kh√¥ng ƒë·ªïi\n",
    "    \"Aggressive\": [0.95, 0.90, 0.85, 0.80, 0.80, 0.80, 0.80, 0.80, 0.80, 0.80]  # Gi·∫£m nhanh\n",
    "}\n",
    "\n",
    "# B·ªè ƒë·ªÉ gi·∫£m th·ªùi gian (~40 ph√∫t):\n",
    "# \"Conservative\": Gi·ªØa Fixed v√† Aggressive, √≠t contrast\n",
    "# \"Linear_Decay\": T∆∞∆°ng t·ª± Aggressive, ch·ªâ kh√°c t·ªëc ƒë·ªô gi·∫£m\n",
    "\n",
    "# Fixed parameters\n",
    "MAX_ITER = 10\n",
    "MIN_NEW_PER_ITER = 20\n",
    "VAL_FRAC = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Output directory\n",
    "RESULTS_DIR = \"data/processed/hybrid_tau_experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff92bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "from src.semi_supervised_library import (\n",
    "    SemiDataConfig, AQI_CLASSES,\n",
    "    time_split, build_feature_columns, _normalize_missing, _align_proba_to_labels\n",
    ")\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "if not (PROJECT_ROOT / \"data\").exists() and (PROJECT_ROOT.parent / \"data\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent.resolve()\n",
    "\n",
    "results_dir = (PROJECT_ROOT / RESULTS_DIR).resolve()\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Results directory: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02956d2",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e826be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet((PROJECT_ROOT / SEMI_DATASET_PATH).resolve())\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Labeled fraction:\", df['is_labeled'].mean())\n",
    "\n",
    "train_df, test_df = time_split(df, cutoff=CUTOFF)\n",
    "print(f\"\\nTrain: {len(train_df):,} samples\")\n",
    "print(f\"  - Labeled: {train_df['is_labeled'].sum():,}\")\n",
    "print(f\"  - Unlabeled: {(~train_df['is_labeled']).sum():,}\")\n",
    "print(f\"Test: {len(test_df):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8906d028",
   "metadata": {},
   "source": [
    "## Visualize œÑ Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658dd010",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = ['steelblue', 'forestgreen', 'coral', 'mediumpurple']\n",
    "schedule_colors = dict(zip(TAU_SCHEDULES.keys(), colors))\n",
    "\n",
    "for schedule_name, tau_values in TAU_SCHEDULES.items():\n",
    "    iterations = list(range(1, len(tau_values) + 1))\n",
    "    ax.plot(iterations, tau_values, marker='o', linewidth=2.5,\n",
    "            label=schedule_name.replace('_', ' '), \n",
    "            color=schedule_colors[schedule_name], alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"Iteration\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"œÑ (Confidence Threshold)\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"œÑ Schedules Over Iterations\", fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='best')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xticks(range(1, 11))\n",
    "ax.set_ylim([0.75, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = results_dir / \"tau_schedules.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved plot: {plot_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775fe05b",
   "metadata": {},
   "source": [
    "## Custom Self-Training with œÑ Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0448763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self_training_with_schedule(df, schedule_name, tau_schedule, max_iter=10):\n",
    "    \"\"\"Self-training with adaptive œÑ per iteration\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SCHEDULE: {schedule_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"œÑ values: {tau_schedule}\")\n",
    "    \n",
    "    data_cfg = SemiDataConfig(cutoff=CUTOFF, random_state=RANDOM_STATE)\n",
    "    \n",
    "    train_df, test_df = time_split(df.copy(), cutoff=CUTOFF)\n",
    "    feat_cols = build_feature_columns(train_df, data_cfg)\n",
    "    \n",
    "    X_all = _normalize_missing(train_df[feat_cols].copy())\n",
    "    y_all = train_df[data_cfg.target_col].astype(\"object\")\n",
    "    \n",
    "    # Split labeled into fit and validation\n",
    "    labeled_idx = train_df.index[pd.notna(y_all)].to_numpy()\n",
    "    unlabeled_idx = train_df.index[pd.isna(y_all)].to_numpy()\n",
    "    \n",
    "    rng = np.random.default_rng(RANDOM_STATE)\n",
    "    rng.shuffle(labeled_idx)\n",
    "    n_val = int(np.floor(VAL_FRAC * labeled_idx.size))\n",
    "    val_idx = labeled_idx[:n_val]\n",
    "    fit_idx = labeled_idx[n_val:]\n",
    "    \n",
    "    # Build pipeline\n",
    "    cat_cols = [c for c in feat_cols if train_df[c].dtype == \"object\"]\n",
    "    \n",
    "    if cat_cols:\n",
    "        encoder = ColumnTransformer([\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols)\n",
    "        ], remainder=\"passthrough\")\n",
    "    else:\n",
    "        encoder = \"passthrough\"\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        (\"encoder\", encoder),\n",
    "        (\"model\", HistGradientBoostingClassifier(\n",
    "            max_iter=100, max_depth=10, learning_rate=0.1, random_state=RANDOM_STATE\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Self-training loop with adaptive œÑ\n",
    "    y_work = y_all.copy()\n",
    "    history = []\n",
    "    \n",
    "    for it in range(1, min(max_iter, len(tau_schedule)) + 1):\n",
    "        tau_current = tau_schedule[it - 1]\n",
    "        \n",
    "        # Fit\n",
    "        pipe.fit(X_all.loc[fit_idx], y_work.loc[fit_idx])\n",
    "        \n",
    "        # Validate\n",
    "        y_val_pred = pipe.predict(X_all.loc[val_idx])\n",
    "        val_acc = float(accuracy_score(y_all.loc[val_idx], y_val_pred))\n",
    "        val_f1 = float(f1_score(y_all.loc[val_idx], y_val_pred, average=\"macro\"))\n",
    "        \n",
    "        # Pseudo-label with current œÑ\n",
    "        if unlabeled_idx.size > 0:\n",
    "            proba_raw = pipe.predict_proba(X_all.loc[unlabeled_idx])\n",
    "            proba = _align_proba_to_labels(proba_raw, pipe.named_steps[\"model\"].classes_, AQI_CLASSES)\n",
    "            max_prob = proba.max(axis=1)\n",
    "            y_hat = np.array(AQI_CLASSES, dtype=object)[proba.argmax(axis=1)]\n",
    "            \n",
    "            pick_mask = max_prob >= tau_current\n",
    "            picked = unlabeled_idx[pick_mask]\n",
    "            picked_labels = y_hat[pick_mask]\n",
    "        else:\n",
    "            picked = np.array([], dtype=int)\n",
    "            picked_labels = np.array([], dtype=object)\n",
    "        \n",
    "        n_new = int(picked.size)\n",
    "        history.append({\n",
    "            \"iter\": it,\n",
    "            \"tau\": float(tau_current),\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"val_f1_macro\": val_f1,\n",
    "            \"unlabeled_pool\": int(unlabeled_idx.size),\n",
    "            \"new_pseudo\": n_new\n",
    "        })\n",
    "        \n",
    "        print(f\"  Iter {it:2d} (œÑ={tau_current:.3f}): Val F1={val_f1:.4f}, New pseudo={n_new:,}, Pool={unlabeled_idx.size:,}\")\n",
    "        \n",
    "        if n_new < MIN_NEW_PER_ITER:\n",
    "            print(f\"  ‚ö†Ô∏è Stopped early: only {n_new} new pseudo-labels\")\n",
    "            break\n",
    "        \n",
    "        # Add pseudo-labels\n",
    "        y_work.loc[picked] = picked_labels\n",
    "        fit_idx = np.unique(np.concatenate([fit_idx, picked]))\n",
    "        \n",
    "        picked_set = set(picked.tolist())\n",
    "        unlabeled_idx = np.array([i for i in unlabeled_idx if i not in picked_set], dtype=int)\n",
    "    \n",
    "    # Test evaluation\n",
    "    X_test = _normalize_missing(test_df[feat_cols].copy())\n",
    "    y_test = test_df[data_cfg.target_col].astype(\"object\")\n",
    "    mask = pd.notna(y_test)\n",
    "    \n",
    "    y_pred = pipe.predict(X_test.loc[mask])\n",
    "    \n",
    "    test_acc = float(accuracy_score(y_test.loc[mask], y_pred))\n",
    "    test_f1 = float(f1_score(y_test.loc[mask], y_pred, average=\"macro\"))\n",
    "    report = classification_report(y_test.loc[mask], y_pred, output_dict=True)\n",
    "    \n",
    "    print(f\"\\n  ‚úÖ Test Results:\")\n",
    "    print(f\"     Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"     F1-macro: {test_f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"schedule_name\": schedule_name,\n",
    "        \"tau_schedule\": tau_schedule,\n",
    "        \"history\": history,\n",
    "        \"test_accuracy\": test_acc,\n",
    "        \"test_f1_macro\": test_f1,\n",
    "        \"per_class_report\": report,\n",
    "        \"total_pseudo_labels\": sum([h[\"new_pseudo\"] for h in history]),\n",
    "        \"iterations_completed\": len(history)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39aa10",
   "metadata": {},
   "source": [
    "## Run Experiments for All Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eef7427",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "print(\"‚ö° Running 2/4 œÑ schedules (optimized for speed)\\n\")\n",
    "\n",
    "for schedule_name, tau_schedule in TAU_SCHEDULES.items():\n",
    "    result = run_self_training_with_schedule(\n",
    "        df=df,\n",
    "        schedule_name=schedule_name,\n",
    "        tau_schedule=tau_schedule,\n",
    "        max_iter=MAX_ITER\n",
    "    )\n",
    "    \n",
    "    results[schedule_name] = result\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SCHEDULE EXPERIMENTS COMPLETED (2/4 schedules)\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a57397c",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42feb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED: Ch·ªâ ch·∫°y 2 schedules c√≥ contrast cao nh·∫•t (gi·∫£m ~50% th·ªùi gian)\n",
    "TAU_SCHEDULES = {\n",
    "    \"Fixed_0.90\": [0.90] * 10,  # Baseline (constant)\n",
    "    \"Aggressive\": [0.95] + [0.90] * 2 + [0.85] * 2 + [0.80] * 5  # Fast decay, th·∫•p nh·∫•t\n",
    "}\n",
    "\n",
    "# B·ªè ƒë·ªÉ gi·∫£m th·ªùi gian (~13-15 ph√∫t m·ªói schedule):\n",
    "# ‚ùå \"Conservative\": Gi·ªØa Fixed v√† Aggressive ‚Üí √≠t contrast\n",
    "# ‚ùå \"Linear_Decay\": T∆∞∆°ng t·ª± Aggressive nh∆∞ng m∆∞·ª£t h∆°n ‚Üí redundant\n",
    "#\n",
    "# L√Ω do ch·ªçn 2 schedules n√†y:\n",
    "# ‚úÖ Fixed 0.90: Baseline (constant œÑ) ƒë·ªÉ so s√°nh\n",
    "# ‚úÖ Aggressive: Extreme adaptive (0.95‚Üí0.80) ‚Üí quan s√°t r√µ ·∫£nh h∆∞·ªüng c·ªßa œÑ decay\n",
    "\n",
    "print(\"‚ö° OPTIMIZATION: Running 2/4 schedules (saves ~25-30 minutes)\")\n",
    "print(\"   ‚úì Fixed 0.90 (baseline, constant œÑ)\")\n",
    "print(\"   ‚úì Aggressive (extreme decay: 0.95‚Üí0.80)\")\n",
    "print(\"   ‚úó Conservative (skipped)\")\n",
    "print(\"   ‚úó Linear Decay (skipped)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7746ae80",
   "metadata": {},
   "source": [
    "## Create Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "for schedule_name, res in results.items():\n",
    "    summary_data.append({\n",
    "        \"Schedule\": schedule_name.replace('_', ' '),\n",
    "        \"Test Accuracy\": res[\"test_accuracy\"],\n",
    "        \"Test F1-macro\": res[\"test_f1_macro\"],\n",
    "        \"Pseudo-labels\": res[\"total_pseudo_labels\"],\n",
    "        \"Iterations\": res[\"iterations_completed\"],\n",
    "        \"Val F1 Peak\": max([h[\"val_f1_macro\"] for h in res[\"history\"]]),\n",
    "        \"Avg œÑ\": np.mean(res[\"tau_schedule\"][:res[\"iterations_completed\"]])\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values(\"Test F1-macro\", ascending=False)\n",
    "\n",
    "print(\"\\nüìä SUMMARY TABLE:\")\n",
    "print(\"=\"*100)\n",
    "display(summary_df)\n",
    "\n",
    "summary_csv = results_dir / \"hybrid_tau_summary.csv\"\n",
    "summary_df.to_csv(summary_csv, index=False)\n",
    "print(f\"\\n‚úÖ Saved summary to: {summary_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b6dfa",
   "metadata": {},
   "source": [
    "## Visualization 1: Test Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "schedules = summary_df[\"Schedule\"].tolist()\n",
    "accuracies = summary_df[\"Test Accuracy\"].tolist()\n",
    "f1_scores = summary_df[\"Test F1-macro\"].tolist()\n",
    "\n",
    "# Accuracy\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(schedules, accuracies, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax1.set_ylabel(\"Test Accuracy\", fontsize=12, fontweight='bold')\n",
    "ax1.set_title(\"Test Accuracy by œÑ Schedule\", fontsize=14, fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim([0.57, 0.61])\n",
    "\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.4f}',\n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# F1-macro\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(schedules, f1_scores, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax2.set_ylabel(\"Test F1-macro\", fontsize=12, fontweight='bold')\n",
    "ax2.set_title(\"Test F1-macro by œÑ Schedule\", fontsize=14, fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_ylim([0.50, 0.56])\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.4f}',\n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = results_dir / \"test_performance_by_schedule.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved plot: {plot_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a553a0",
   "metadata": {},
   "source": [
    "## Visualization 2: Validation Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618a9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for schedule_name, res in results.items():\n",
    "    history = res[\"history\"]\n",
    "    iterations = [h[\"iter\"] for h in history]\n",
    "    val_f1 = [h[\"val_f1_macro\"] for h in history]\n",
    "    color = schedule_colors[schedule_name]\n",
    "    \n",
    "    ax.plot(iterations, val_f1, marker='o', linewidth=2.5,\n",
    "            label=schedule_name.replace('_', ' '), color=color, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"Iteration\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"Validation F1-macro\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Validation Learning Curves by œÑ Schedule\", fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='best')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xticks(range(1, 11))\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = results_dir / \"validation_curves_by_schedule.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved plot: {plot_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69391914",
   "metadata": {},
   "source": [
    "## Visualization 3: Pseudo-labeling Activity Over Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ab298",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for schedule_name, res in results.items():\n",
    "    history = res[\"history\"]\n",
    "    iterations = [h[\"iter\"] for h in history]\n",
    "    new_pseudo = [h[\"new_pseudo\"] for h in history]\n",
    "    color = schedule_colors[schedule_name]\n",
    "    \n",
    "    ax.plot(iterations, new_pseudo, marker='s', linewidth=2,\n",
    "            label=schedule_name.replace('_', ' '), color=color, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"Iteration\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"New Pseudo-labels Added\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Pseudo-labeling Activity by œÑ Schedule\", fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='best')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xticks(range(1, 11))\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = results_dir / \"pseudo_labeling_activity.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved plot: {plot_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e241a1",
   "metadata": {},
   "source": [
    "## Visualization 4: Total Pseudo-labels Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "schedules_list = summary_df[\"Schedule\"].tolist()\n",
    "pseudo_totals = summary_df[\"Pseudo-labels\"].tolist()\n",
    "\n",
    "bars = ax.bar(schedules_list, pseudo_totals, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.set_ylabel(\"Total Pseudo-labels Added\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Total Pseudo-labeling by œÑ Schedule\", fontsize=14, fontweight='bold')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height):,}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = results_dir / \"total_pseudo_labels.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved plot: {plot_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc0708",
   "metadata": {},
   "source": [
    "## Visualization 5: œÑ vs Performance Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c2d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Scatter plot: Avg œÑ vs F1\n",
    "ax1 = axes[0]\n",
    "avg_taus = summary_df[\"Avg œÑ\"].tolist()\n",
    "f1_list = summary_df[\"Test F1-macro\"].tolist()\n",
    "\n",
    "for i, schedule in enumerate(schedules_list):\n",
    "    ax1.scatter(avg_taus[i], f1_list[i], s=200, c=colors[i], \n",
    "                alpha=0.8, edgecolor='black', linewidth=2)\n",
    "    ax1.text(avg_taus[i], f1_list[i] + 0.002, schedule,\n",
    "             ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax1.set_xlabel(\"Average œÑ\", fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel(\"Test F1-macro\", fontsize=12, fontweight='bold')\n",
    "ax1.set_title(\"Average œÑ vs Test Performance\", fontsize=14, fontweight='bold')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Bar plot: Pseudo-labels vs F1\n",
    "ax2 = axes[1]\n",
    "x = np.arange(len(schedules_list))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, [p/1000 for p in pseudo_totals], width, \n",
    "                label='Pseudo-labels (K)', color='skyblue', alpha=0.8)\n",
    "ax2_twin = ax2.twinx()\n",
    "bars2 = ax2_twin.bar(x + width/2, f1_list, width,\n",
    "                     label='F1-macro', color='coral', alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel(\"Schedule\", fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel(\"Pseudo-labels (thousands)\", fontsize=11, fontweight='bold', color='skyblue')\n",
    "ax2_twin.set_ylabel(\"Test F1-macro\", fontsize=11, fontweight='bold', color='coral')\n",
    "ax2.set_title(\"Pseudo-labels vs Performance Trade-off\", fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(schedules_list, rotation=45, ha='right')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = results_dir / \"tau_performance_correlation.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved plot: {plot_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca4b8c",
   "metadata": {},
   "source": [
    "## Analysis & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä KEY FINDINGS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "best_idx = summary_df[\"Test F1-macro\"].idxmax()\n",
    "best_schedule = summary_df.loc[best_idx]\n",
    "\n",
    "print(f\"\\nüèÜ Best Schedule: {best_schedule['Schedule']}\")\n",
    "print(f\"   Test F1-macro: {best_schedule['Test F1-macro']:.4f}\")\n",
    "print(f\"   Test Accuracy: {best_schedule['Test Accuracy']:.4f}\")\n",
    "print(f\"   Average œÑ: {best_schedule['Avg œÑ']:.3f}\")\n",
    "print(f\"   Pseudo-labels: {best_schedule['Pseudo-labels']:,}\")\n",
    "print(f\"   Val F1 Peak: {best_schedule['Val F1 Peak']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìà Ranking by F1-macro:\")\n",
    "for i, row in summary_df.iterrows():\n",
    "    print(f\"   {i+1}. {row['Schedule']}: {row['Test F1-macro']:.4f} (avg œÑ={row['Avg œÑ']:.3f})\")\n",
    "\n",
    "print(f\"\\nüí° Schedule Behaviors:\")\n",
    "baseline_f1 = summary_df[summary_df[\"Schedule\"] == \"Fixed 0.90\"][\"Test F1-macro\"].values[0]\n",
    "for i, row in summary_df.iterrows():\n",
    "    improvement = (row[\"Test F1-macro\"] - baseline_f1) * 100\n",
    "    print(f\"   {row['Schedule']}:\")\n",
    "    print(f\"      - F1 improvement vs Fixed: {improvement:+.2f}%\")\n",
    "    print(f\"      - Pseudo-labels: {row['Pseudo-labels']:,}\")\n",
    "    print(f\"      - Efficiency: {row['Test F1-macro'] / (row['Pseudo-labels']/1000):.5f} F1 per 1K labels\")\n",
    "\n",
    "print(f\"\\nüéØ Conclusion:\")\n",
    "if best_schedule[\"Test F1-macro\"] > baseline_f1:\n",
    "    print(f\"   ‚úÖ Hybrid œÑ strategy IMPROVES over fixed œÑ!\")\n",
    "    print(f\"      Best: {best_schedule['Schedule']}\")\n",
    "    print(f\"      Improvement: +{(best_schedule['Test F1-macro'] - baseline_f1)*100:.2f}%\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Fixed œÑ=0.90 remains competitive\")\n",
    "    print(f\"      Hybrid strategies not significantly better\")\n",
    "\n",
    "print(f\"\\n‚úÖ All visualizations saved to: {results_dir}\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd98744b",
   "metadata": {},
   "source": [
    "## Dashboard Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58173929",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_data = {\n",
    "    \"experiment_type\": \"hybrid_tau_schedule\",\n",
    "    \"parameters\": {\n",
    "        \"max_iter\": MAX_ITER,\n",
    "        \"schedules\": {k: v for k, v in TAU_SCHEDULES.items()}\n",
    "    },\n",
    "    \"summary\": summary_df.to_dict(orient='records'),\n",
    "    \"best_schedule\": {\n",
    "        \"name\": best_schedule[\"Schedule\"],\n",
    "        \"f1_macro\": float(best_schedule[\"Test F1-macro\"]),\n",
    "        \"accuracy\": float(best_schedule[\"Test Accuracy\"]),\n",
    "        \"avg_tau\": float(best_schedule[\"Avg œÑ\"])\n",
    "    },\n",
    "    \"baseline_comparison\": {\n",
    "        \"fixed_tau_f1\": float(baseline_f1),\n",
    "        \"best_hybrid_f1\": float(best_schedule[\"Test F1-macro\"]),\n",
    "        \"improvement\": float((best_schedule[\"Test F1-macro\"] - baseline_f1) / baseline_f1 * 100)\n",
    "    },\n",
    "    \"visualizations\": [\n",
    "        \"tau_schedules.png\",\n",
    "        \"test_performance_by_schedule.png\",\n",
    "        \"validation_curves_by_schedule.png\",\n",
    "        \"pseudo_labeling_activity.png\",\n",
    "        \"total_pseudo_labels.png\",\n",
    "        \"tau_performance_correlation.png\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "dashboard_file = results_dir / \"dashboard_summary.json\"\n",
    "with open(dashboard_file, \"w\") as f:\n",
    "    json.dump(dashboard_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Dashboard summary saved to: {dashboard_file}\")\n",
    "display(Markdown(f\"## Experiment Complete! ‚úÖ\\n\\nResults: `{results_dir}`\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
