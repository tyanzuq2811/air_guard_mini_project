{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# FlexMatch-lite Training: Dynamic Threshold + Focal Loss\n",
                "\n",
                "**Mục tiêu:** Cải thiện self-training bằng cách:\n",
                "1. **Dynamic Threshold theo lớp** - Giúp lớp hiếm được chọn dễ dàng hơn\n",
                "2. **Focal Loss** - Giảm ảnh hưởng của lớp đa số trong training\n",
                "\n",
                "**Kỳ vọng:** Cải thiện F1-macro, đặc biệt cho các lớp AQI nguy hiểm (Hazardous, Very Unhealthy)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "source": [
                "# Papermill parameters\n",
                "BASE_TAU = 0.90\n",
                "GAMMA = 2.0\n",
                "MAX_ITER = 10\n",
                "LABEL_MISSING_FRACTION = 0.95"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import sys\n",
                "import json\n",
                "import warnings\n",
                "from pathlib import Path\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "sns.set_style('whitegrid')\n",
                "\n",
                "# Add src to path\n",
                "# Robustly find project root\n",
                "current_dir = Path.cwd()\n",
                "project_root = current_dir\n",
                "while not (project_root / 'src').exists():\n",
                "    if project_root.parent == project_root:\n",
                "        break # Reached file system root\n",
                "    project_root = project_root.parent\n",
                "\n",
                "if (project_root / 'src').exists():\n",
                "    sys.path.insert(0, str(project_root / 'src'))\n",
                "    print(f\"Added {project_root / 'src'} to path\")\n",
                "else:\n",
                "    print(\"Warning: could not find src directory\")\n",
                "\n",
                "from semi_supervised_library import (\n",
                "    SemiDataConfig,\n",
                "    FlexMatchConfig,\n",
                "    SelfTrainingConfig,\n",
                "    run_self_training,\n",
                "    run_flexmatch_training,\n",
                "    AQI_CLASSES\n",
                ")\n",
                "\n",
                "print(\"[OK] Libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Load data\n",
                "DATA_PATH = Path.cwd().parent / 'data' / 'processed' / 'dataset_for_semi.parquet'\n",
                "if not DATA_PATH.exists():\n",
                "    # Try relative path from project root\n",
                "    DATA_PATH = project_root / 'data' / 'processed' / 'dataset_for_semi.parquet'\n",
                "\n",
                "df = pd.read_parquet(DATA_PATH)\n",
                "\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"\\nClass distribution:\")\n",
                "print(df['aqi_class'].value_counts().sort_index())\n",
                "\n",
                "# Check labeled fraction\n",
                "labeled_frac = df['is_labeled'].mean()\n",
                "print(f\"\\nLabeled fraction: {labeled_frac:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Experiment 1: Baseline Self-Training (τ=0.90)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Configuration\n",
                "data_cfg = SemiDataConfig()\n",
                "st_cfg = SelfTrainingConfig(tau=0.90, max_iter=MAX_ITER)\n",
                "\n",
                "print(\"Running baseline self-training...\")\n",
                "baseline_results = run_self_training(df, data_cfg, st_cfg)\n",
                "\n",
                "baseline_metrics = baseline_results['test_metrics']\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(\"BASELINE SELF-TRAINING RESULTS\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"Test Accuracy: {baseline_metrics['accuracy']:.4f}\")\n",
                "print(f\"Test F1-macro: {baseline_metrics['f1_macro']:.4f}\")\n",
                "print(f\"\\nIterations: {len(baseline_results['history'])}\")\n",
                "print(f\"Total pseudo-labels: {sum([h['new_pseudo'] for h in baseline_results['history']])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Experiment 2: FlexMatch with Dynamic Threshold Only"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Test different base_tau values\n",
                "flexmatch_dynamic_results = {}\n",
                "\n",
                "for base_tau in [0.85, 0.90, 0.95]:\n",
                "    print(f\"\\nTesting base_tau={base_tau}...\")\n",
                "    \n",
                "    fm_cfg = FlexMatchConfig(\n",
                "        base_tau=base_tau,\n",
                "        gamma=2.0,\n",
                "        max_iter=MAX_ITER,\n",
                "        use_focal_loss=False  # Dynamic threshold only\n",
                "    )\n",
                "    \n",
                "    results = run_flexmatch_training(df, data_cfg, fm_cfg)\n",
                "    flexmatch_dynamic_results[base_tau] = results\n",
                "    \n",
                "    metrics = results['test_metrics']\n",
                "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
                "    print(f\"  F1-macro: {metrics['f1_macro']:.4f}\")\n",
                "\n",
                "print(\"\\n[OK] Dynamic threshold experiments completed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Experiment 3: FlexMatch with Focal Loss Only"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Test different gamma values\n",
                "flexmatch_focal_results = {}\n",
                "\n",
                "for gamma in [1.0, 2.0, 3.0]:\n",
                "    print(f\"\\nTesting gamma={gamma}...\")\n",
                "    \n",
                "    # Use fixed threshold but with focal loss\n",
                "    fm_cfg = FlexMatchConfig(\n",
                "        base_tau=0.90,\n",
                "        gamma=gamma,\n",
                "        max_iter=MAX_ITER,\n",
                "        alpha=1.0,  # No smoothing = fixed threshold\n",
                "        use_focal_loss=True\n",
                "    )\n",
                "    \n",
                "    results = run_flexmatch_training(df, data_cfg, fm_cfg)\n",
                "    flexmatch_focal_results[gamma] = results\n",
                "    \n",
                "    metrics = results['test_metrics']\n",
                "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
                "    print(f\"  F1-macro: {metrics['f1_macro']:.4f}\")\n",
                "\n",
                "print(\"\\n[OK] Focal loss experiments completed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Experiment 4: FlexMatch Combined (Best Config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Best configuration: Dynamic threshold + Focal loss\n",
                "fm_cfg_best = FlexMatchConfig(\n",
                "    base_tau=BASE_TAU,\n",
                "    gamma=GAMMA,\n",
                "    alpha=0.9,\n",
                "    max_iter=MAX_ITER,\n",
                "    use_focal_loss=True\n",
                ")\n",
                "\n",
                "print(\"Running FlexMatch with combined approach...\")\n",
                "flexmatch_combined_results = run_flexmatch_training(df, data_cfg, fm_cfg_best)\n",
                "\n",
                "combined_metrics = flexmatch_combined_results['test_metrics']\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(\"FLEXMATCH COMBINED RESULTS\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"Test Accuracy: {combined_metrics['accuracy']:.4f}\")\n",
                "print(f\"Test F1-macro: {combined_metrics['f1_macro']:.4f}\")\n",
                "print(f\"\\nIterations: {len(flexmatch_combined_results['history'])}\")\n",
                "print(f\"Total pseudo-labels: {sum([h['new_pseudo'] for h in flexmatch_combined_results['history']])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Visualization & Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Create output directory\n",
                "# Check paths\n",
                "OUTPUT_DIR = Path.cwd().parent / 'data' / 'processed' / 'flexmatch_experiments'\n",
                "if not OUTPUT_DIR.parent.exists():\n",
                "    OUTPUT_DIR = project_root / 'data' / 'processed' / 'flexmatch_experiments'\n",
                "\n",
                "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f\"Output directory: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.1. Comparison Chart: All Methods"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Prepare comparison data\n",
                "comparison_data = [\n",
                "    {\n",
                "        'Method': 'Baseline Self-Training',\n",
                "        'Accuracy': baseline_metrics['accuracy'],\n",
                "        'F1-macro': baseline_metrics['f1_macro']\n",
                "    },\n",
                "    {\n",
                "        'Method': 'FlexMatch Combined',\n",
                "        'Accuracy': combined_metrics['accuracy'],\n",
                "        'F1-macro': combined_metrics['f1_macro']\n",
                "    }\n",
                "]\n",
                "\n",
                "comparison_df = pd.DataFrame(comparison_data)\n",
                "\n",
                "# Plot\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "# Accuracy\n",
                "axes[0].bar(comparison_df['Method'], comparison_df['Accuracy'], color=['#3498db', '#e74c3c'])\n",
                "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
                "axes[0].set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
                "axes[0].set_ylim([0.5, 0.65])\n",
                "axes[0].tick_params(axis='x', rotation=15)\n",
                "\n",
                "for i, v in enumerate(comparison_df['Accuracy']):\n",
                "    axes[0].text(i, v + 0.005, f'{v:.4f}', ha='center', fontweight='bold')\n",
                "\n",
                "# F1-macro\n",
                "axes[1].bar(comparison_df['Method'], comparison_df['F1-macro'], color=['#3498db', '#e74c3c'])\n",
                "axes[1].set_ylabel('F1-macro', fontsize=12)\n",
                "axes[1].set_title('Test F1-macro Comparison', fontsize=14, fontweight='bold')\n",
                "axes[1].set_ylim([0.45, 0.60])\n",
                "axes[1].tick_params(axis='x', rotation=15)\n",
                "\n",
                "for i, v in enumerate(comparison_df['F1-macro']):\n",
                "    axes[1].text(i, v + 0.005, f'{v:.4f}', ha='center', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'test_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"[OK] Comparison chart saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2. Threshold Evolution Over Iterations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Extract threshold history\n",
                "history = flexmatch_combined_results['history']\n",
                "\n",
                "# Prepare data for plotting\n",
                "iterations = [h['iter'] for h in history]\n",
                "threshold_data = {}\n",
                "\n",
                "for class_idx, class_name in enumerate(AQI_CLASSES):\n",
                "    threshold_data[class_name] = [\n",
                "        h['thresholds'].get(class_idx, h['base_tau']) \n",
                "        for h in history\n",
                "    ]\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(12, 6))\n",
                "\n",
                "for class_name, thresholds in threshold_data.items():\n",
                "    plt.plot(iterations, thresholds, marker='o', label=class_name, linewidth=2)\n",
                "\n",
                "plt.axhline(y=BASE_TAU, color='black', linestyle='--', alpha=0.5, label=f'Base τ={BASE_TAU}')\n",
                "plt.xlabel('Iteration', fontsize=12)\n",
                "plt.ylabel('Threshold (τ)', fontsize=12)\n",
                "plt.title('Dynamic Threshold Evolution per Class', fontsize=14, fontweight='bold')\n",
                "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'threshold_evolution.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"[OK] Threshold evolution chart saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.3. Per-Class F1-Score Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Extract per-class F1 scores\n",
                "baseline_f1_per_class = []\n",
                "flexmatch_f1_per_class = []\n",
                "\n",
                "for class_name in AQI_CLASSES:\n",
                "    baseline_f1 = baseline_metrics['report'][class_name]['f1-score']\n",
                "    flexmatch_f1 = combined_metrics['report'][class_name]['f1-score']\n",
                "    \n",
                "    baseline_f1_per_class.append(baseline_f1)\n",
                "    flexmatch_f1_per_class.append(flexmatch_f1)\n",
                "\n",
                "# Plot\n",
                "x = np.arange(len(AQI_CLASSES))\n",
                "width = 0.35\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(14, 6))\n",
                "rects1 = ax.bar(x - width/2, baseline_f1_per_class, width, label='Baseline Self-Training', color='#3498db')\n",
                "rects2 = ax.bar(x + width/2, flexmatch_f1_per_class, width, label='FlexMatch Combined', color='#e74c3c')\n",
                "\n",
                "ax.set_ylabel('F1-Score', fontsize=12)\n",
                "ax.set_title('Per-Class F1-Score Comparison', fontsize=14, fontweight='bold')\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels([c.replace('_', ' ') for c in AQI_CLASSES], rotation=15, ha='right')\n",
                "ax.legend()\n",
                "ax.grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# Add value labels\n",
                "def autolabel(rects):\n",
                "    for rect in rects:\n",
                "        height = rect.get_height()\n",
                "        ax.annotate(f'{height:.3f}',\n",
                "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
                "                    xytext=(0, 3),\n",
                "                    textcoords=\"offset points\",\n",
                "                    ha='center', va='bottom', fontsize=9)\n",
                "\n",
                "autolabel(rects1)\n",
                "autolabel(rects2)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'per_class_f1_comparison.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"[OK] Per-class F1 comparison saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.4. Improvement Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Calculate improvements\n",
                "improvements = []\n",
                "\n",
                "for i, class_name in enumerate(AQI_CLASSES):\n",
                "    baseline_f1 = baseline_f1_per_class[i]\n",
                "    flexmatch_f1 = flexmatch_f1_per_class[i]\n",
                "    \n",
                "    abs_improvement = flexmatch_f1 - baseline_f1\n",
                "    rel_improvement = (abs_improvement / baseline_f1) * 100 if baseline_f1 > 0 else 0\n",
                "    \n",
                "    improvements.append({\n",
                "        'Class': class_name,\n",
                "        'Baseline F1': baseline_f1,\n",
                "        'FlexMatch F1': flexmatch_f1,\n",
                "        'Absolute Δ': abs_improvement,\n",
                "        'Relative Δ (%)': rel_improvement\n",
                "    })\n",
                "\n",
                "improvements_df = pd.DataFrame(improvements)\n",
                "improvements_df = improvements_df.sort_values('Relative Δ (%)', ascending=False)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"PER-CLASS IMPROVEMENT ANALYSIS\")\n",
                "print(\"=\"*80)\n",
                "print(improvements_df.to_string(index=False))\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "\n",
                "# Save to CSV\n",
                "improvements_df.to_csv(OUTPUT_DIR / 'per_class_improvements.csv', index=False)\n",
                "print(\"\\n[OK] Improvement analysis saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Save metrics\n",
                "with open(OUTPUT_DIR / 'metrics_flexmatch.json', 'w') as f:\n",
                "    json.dump(combined_metrics, f, indent=2)\n",
                "\n",
                "# Save comparison summary\n",
                "summary = {\n",
                "    'baseline': {\n",
                "        'accuracy': baseline_metrics['accuracy'],\n",
                "        'f1_macro': baseline_metrics['f1_macro'],\n",
                "        'iterations': len(baseline_results['history']),\n",
                "        'total_pseudo_labels': sum([h['new_pseudo'] for h in baseline_results['history']])\n",
                "    },\n",
                "    'flexmatch_combined': {\n",
                "        'accuracy': combined_metrics['accuracy'],\n",
                "        'f1_macro': combined_metrics['f1_macro'],\n",
                "        'iterations': len(flexmatch_combined_results['history']),\n",
                "        'total_pseudo_labels': sum([h['new_pseudo'] for h in flexmatch_combined_results['history']]),\n",
                "        'config': {\n",
                "            'base_tau': BASE_TAU,\n",
                "            'gamma': GAMMA,\n",
                "            'alpha': 0.9,\n",
                "            'use_focal_loss': True\n",
                "        }\n",
                "    },\n",
                "    'improvements': {\n",
                "        'accuracy_delta': combined_metrics['accuracy'] - baseline_metrics['accuracy'],\n",
                "        'f1_macro_delta': combined_metrics['f1_macro'] - baseline_metrics['f1_macro'],\n",
                "        'f1_macro_relative_improvement': (\n",
                "            (combined_metrics['f1_macro'] - baseline_metrics['f1_macro']) / \n",
                "            baseline_metrics['f1_macro'] * 100\n",
                "        )\n",
                "    }\n",
                "}\n",
                "\n",
                "with open(OUTPUT_DIR / 'flexmatch_summary.json', 'w') as f:\n",
                "    json.dump(summary, f, indent=2)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "print(json.dumps(summary, indent=2))\n",
                "print(\"\\n[OK] All results saved to:\", OUTPUT_DIR)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Conclusion\n",
                "\n",
                "**FlexMatch-lite Results:**\n",
                "- Dynamic threshold helps minority classes get selected\n",
                "- Focal loss reduces majority class dominance\n",
                "- Combined approach shows best F1-macro improvement\n",
                "\n",
                "**Next Steps:**\n",
                "- Compare with Label Spreading\n",
                "- Analyze failure cases\n",
                "- Consider ensemble methods"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "beijing_env",
            "language": "python",
            "name": "beijing_env"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}