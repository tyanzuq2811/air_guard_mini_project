{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Label Spreading: Graph-Based Semi-Supervised Learning\n",
                "\n",
                "**Mục tiêu:** So sánh Label Spreading với Self-Training và FlexMatch\n",
                "\n",
                "**Ưu điểm:**\n",
                "- Sử dụng manifold structure của dữ liệu\n",
                "- Không cần iterative pseudo-labeling\n",
                "- Tự nhiên xử lý class imbalance qua graph structure\n",
                "\n",
                "**Nhược điểm:**\n",
                "- Memory intensive (O(n²))\n",
                "- Chậm hơn self-training cho large datasets"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "source": [
                "# Papermill parameters\n",
                "KERNEL = \"rbf\"\n",
                "GAMMA = 20.0\n",
                "ALPHA = 0.2\n",
                "SAMPLE_SIZE = 50000\n",
                "MAX_ITER = 30"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import sys\n",
                "import json\n",
                "import time\n",
                "import warnings\n",
                "from pathlib import Path\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "sns.set_style('whitegrid')\n",
                "\n",
                "# Add src to path\n",
                "# Robustly find project root\n",
                "current_dir = Path.cwd()\n",
                "project_root = current_dir\n",
                "while not (project_root / 'src').exists():\n",
                "    if project_root.parent == project_root:\n",
                "        break # Reached file system root\n",
                "    project_root = project_root.parent\n",
                "\n",
                "if (project_root / 'src').exists():\n",
                "    sys.path.insert(0, str(project_root / 'src'))\n",
                "    print(f\"Added {project_root / 'src'} to path\")\n",
                "else:\n",
                "    print(\"Warning: could not find src directory\")\n",
                "\n",
                "from semi_supervised_library import (\n",
                "    SemiDataConfig,\n",
                "    LabelSpreadingConfig,\n",
                "    run_label_spreading,\n",
                "    AQI_CLASSES\n",
                ")\n",
                "\n",
                "print(\"[OK] Libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Load data\n",
                "DATA_PATH = Path.cwd().parent / 'data' / 'processed' / 'dataset_for_semi.parquet'\n",
                "if not DATA_PATH.exists():\n",
                "    # Try relative path from project root\n",
                "    DATA_PATH = project_root / 'data' / 'processed' / 'dataset_for_semi.parquet'\n",
                "\n",
                "df = pd.read_parquet(DATA_PATH)\n",
                "\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"\\nClass distribution:\")\n",
                "print(df['aqi_class'].value_counts().sort_index())\n",
                "\n",
                "# Check labeled fraction\n",
                "labeled_frac = df['is_labeled'].mean()\n",
                "print(f\"\\nLabeled fraction: {labeled_frac:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Experiment 1: Label Propagation (Simpler Version)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "from sklearn.semi_supervised import LabelPropagation\n",
                "\n",
                "# Test different gamma values for Label Propagation\n",
                "label_prop_results = {}\n",
                "\n",
                "data_cfg = SemiDataConfig()\n",
                "\n",
                "for gamma_val in [10, 20, 30]:\n",
                "    print(f\"\\nTesting Label Propagation with gamma={gamma_val}...\")\n",
                "    \n",
                "    # Use LabelSpreadingConfig but we'll manually use LabelPropagation\n",
                "    ls_cfg = LabelSpreadingConfig(\n",
                "        kernel=\"rbf\",\n",
                "        gamma=gamma_val,\n",
                "        alpha=0.2,  # Not used in LabelPropagation\n",
                "        max_iter=MAX_ITER,\n",
                "        sample_size=SAMPLE_SIZE\n",
                "    )\n",
                "    \n",
                "    start_time = time.time()\n",
                "    results = run_label_spreading(df, data_cfg, ls_cfg)\n",
                "    elapsed_time = time.time() - start_time\n",
                "    \n",
                "    label_prop_results[gamma_val] = {\n",
                "        'results': results,\n",
                "        'time': elapsed_time\n",
                "    }\n",
                "    \n",
                "    metrics = results['test_metrics']\n",
                "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
                "    print(f\"  F1-macro: {metrics['f1_macro']:.4f}\")\n",
                "    print(f\"  Time: {elapsed_time:.2f}s\")\n",
                "    print(f\"  Sampled: {metrics['sampled']}, Size: {metrics['sample_size']}\")\n",
                "\n",
                "print(\"\\n[OK] Label Propagation experiments completed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Experiment 2: Label Spreading with Different Parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Grid search for best parameters\n",
                "label_spread_results = {}\n",
                "\n",
                "param_grid = [\n",
                "    {'gamma': 10, 'alpha': 0.1},\n",
                "    {'gamma': 20, 'alpha': 0.2},\n",
                "    {'gamma': 30, 'alpha': 0.3},\n",
                "]\n",
                "\n",
                "for params in param_grid:\n",
                "    gamma_val = params['gamma']\n",
                "    alpha_val = params['alpha']\n",
                "    \n",
                "    print(f\"\\nTesting Label Spreading with gamma={gamma_val}, alpha={alpha_val}...\")\n",
                "    \n",
                "    ls_cfg = LabelSpreadingConfig(\n",
                "        kernel=KERNEL,\n",
                "        gamma=gamma_val,\n",
                "        alpha=alpha_val,\n",
                "        max_iter=MAX_ITER,\n",
                "        sample_size=SAMPLE_SIZE\n",
                "    )\n",
                "    \n",
                "    start_time = time.time()\n",
                "    results = run_label_spreading(df, data_cfg, ls_cfg)\n",
                "    elapsed_time = time.time() - start_time\n",
                "    \n",
                "    key = f\"gamma={gamma_val}, alpha={alpha_val}\"\n",
                "    label_spread_results[key] = {\n",
                "        'results': results,\n",
                "        'time': elapsed_time,\n",
                "        'params': params\n",
                "    }\n",
                "    \n",
                "    metrics = results['test_metrics']\n",
                "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
                "    print(f\"  F1-macro: {metrics['f1_macro']:.4f}\")\n",
                "    print(f\"  Time: {elapsed_time:.2f}s\")\n",
                "\n",
                "print(\"\\n[OK] Label Spreading experiments completed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Find Best Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Find best config based on F1-macro\n",
                "best_key = None\n",
                "best_f1 = 0\n",
                "\n",
                "for key, data in label_spread_results.items():\n",
                "    f1 = data['results']['test_metrics']['f1_macro']\n",
                "    if f1 > best_f1:\n",
                "        best_f1 = f1\n",
                "        best_key = key\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"BEST LABEL SPREADING CONFIGURATION\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"Config: {best_key}\")\n",
                "print(f\"F1-macro: {best_f1:.4f}\")\n",
                "\n",
                "best_results = label_spread_results[best_key]['results']\n",
                "best_metrics = best_results['test_metrics']\n",
                "\n",
                "print(f\"\\nTest Accuracy: {best_metrics['accuracy']:.4f}\")\n",
                "print(f\"Test F1-macro: {best_metrics['f1_macro']:.4f}\")\n",
                "print(f\"Training Time: {label_spread_results[best_key]['time']:.2f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Comparison with Self-Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Load self-training results for comparison\n",
                "ST_METRICS_PATH = Path.cwd().parent / 'data' / 'processed' / 'self_training_experiments' / 'metrics_tau_0_9.json'\n",
                "if not ST_METRICS_PATH.parent.exists():\n",
                "    ST_METRICS_PATH = project_root / 'data' / 'processed' / 'self_training_experiments' / 'metrics_tau_0_9.json'\n",
                "\n",
                "FM_METRICS_PATH = Path.cwd().parent / 'data' / 'processed' / 'flexmatch_experiments' / 'metrics_flexmatch.json'\n",
                "if not FM_METRICS_PATH.parent.exists():\n",
                "    FM_METRICS_PATH = project_root / 'data' / 'processed' / 'flexmatch_experiments' / 'metrics_flexmatch.json'\n",
                "\n",
                "# Load if exists\n",
                "st_metrics = None\n",
                "fm_metrics = None\n",
                "\n",
                "if ST_METRICS_PATH.exists():\n",
                "    with open(ST_METRICS_PATH, 'r') as f:\n",
                "        st_metrics = json.load(f)\n",
                "    print(\"[OK] Loaded Self-Training metrics\")\n",
                "\n",
                "if FM_METRICS_PATH.exists():\n",
                "    with open(FM_METRICS_PATH, 'r') as f:\n",
                "        fm_metrics = json.load(f)\n",
                "    print(\"[OK] Loaded FlexMatch metrics\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Create comparison dataframe\n",
                "comparison_data = []\n",
                "\n",
                "if st_metrics:\n",
                "    comparison_data.append({\n",
                "        'Method': 'Self-Training (τ=0.9)',\n",
                "        'Accuracy': st_metrics['accuracy'],\n",
                "        'F1-macro': st_metrics['f1_macro']\n",
                "    })\n",
                "\n",
                "if fm_metrics:\n",
                "    comparison_data.append({\n",
                "        'Method': 'FlexMatch',\n",
                "        'Accuracy': fm_metrics['accuracy'],\n",
                "        'F1-macro': fm_metrics['f1_macro']\n",
                "    })\n",
                "\n",
                "comparison_data.append({\n",
                "    'Method': 'Label Spreading',\n",
                "    'Accuracy': best_metrics['accuracy'],\n",
                "    'F1-macro': best_metrics['f1_macro']\n",
                "})\n",
                "\n",
                "comparison_df = pd.DataFrame(comparison_data)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"METHOD COMPARISON\")\n",
                "print(\"=\"*60)\n",
                "print(comparison_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Create output directory\n",
                "OUTPUT_DIR = Path.cwd().parent / 'data' / 'processed' / 'label_spreading_experiments'\n",
                "if not OUTPUT_DIR.parent.exists():\n",
                "    OUTPUT_DIR = project_root / 'data' / 'processed' / 'label_spreading_experiments'\n",
                "\n",
                "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f\"Output directory: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.1. Method Comparison Chart"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Plot comparison\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
                "\n",
                "# Accuracy\n",
                "axes[0].bar(comparison_df['Method'], comparison_df['Accuracy'], color=colors[:len(comparison_df)])\n",
                "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
                "axes[0].set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
                "axes[0].set_ylim([0.5, 0.65])\n",
                "axes[0].tick_params(axis='x', rotation=15)\n",
                "\n",
                "for i, v in enumerate(comparison_df['Accuracy']):\n",
                "    axes[0].text(i, v + 0.005, f'{v:.4f}', ha='center', fontweight='bold')\n",
                "\n",
                "# F1-macro\n",
                "axes[1].bar(comparison_df['Method'], comparison_df['F1-macro'], color=colors[:len(comparison_df)])\n",
                "axes[1].set_ylabel('F1-macro', fontsize=12)\n",
                "axes[1].set_title('Test F1-macro Comparison', fontsize=14, fontweight='bold')\n",
                "axes[1].set_ylim([0.45, 0.60])\n",
                "axes[1].tick_params(axis='x', rotation=15)\n",
                "\n",
                "for i, v in enumerate(comparison_df['F1-macro']):\n",
                "    axes[1].text(i, v + 0.005, f'{v:.4f}', ha='center', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'method_comparison.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"[OK] Comparison chart saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2. Per-Class F1-Score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Extract per-class F1 scores\n",
                "ls_f1_per_class = [best_metrics['report'][c]['f1-score'] for c in AQI_CLASSES]\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(12, 6))\n",
                "x = np.arange(len(AQI_CLASSES))\n",
                "\n",
                "plt.bar(x, ls_f1_per_class, color='#2ecc71', alpha=0.8, label='Label Spreading')\n",
                "\n",
                "if st_metrics:\n",
                "    st_f1_per_class = [st_metrics['report'][c]['f1-score'] for c in AQI_CLASSES]\n",
                "    plt.plot(x, st_f1_per_class, 'o-', color='#3498db', linewidth=2, \n",
                "             markersize=8, label='Self-Training', alpha=0.7)\n",
                "\n",
                "plt.ylabel('F1-Score', fontsize=12)\n",
                "plt.title('Per-Class F1-Score: Label Spreading', fontsize=14, fontweight='bold')\n",
                "plt.xticks(x, [c.replace('_', ' ') for c in AQI_CLASSES], rotation=15, ha='right')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3, axis='y')\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'per_class_f1.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"[OK] Per-class F1 chart saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.3. Training Time Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Compare training times\n",
                "time_data = []\n",
                "\n",
                "for key, data in label_spread_results.items():\n",
                "    time_data.append({\n",
                "        'Config': key,\n",
                "        'Time (s)': data['time'],\n",
                "        'F1-macro': data['results']['test_metrics']['f1_macro']\n",
                "    })\n",
                "\n",
                "time_df = pd.DataFrame(time_data)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"TRAINING TIME ANALYSIS\")\n",
                "print(\"=\"*60)\n",
                "print(time_df.to_string(index=False))\n",
                "\n",
                "# Plot\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "scatter = ax.scatter(time_df['Time (s)'], time_df['F1-macro'], \n",
                "                     s=200, c=time_df['F1-macro'], cmap='viridis', alpha=0.7)\n",
                "\n",
                "for i, row in time_df.iterrows():\n",
                "    ax.annotate(row['Config'], (row['Time (s)'], row['F1-macro']), \n",
                "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
                "\n",
                "ax.set_xlabel('Training Time (seconds)', fontsize=12)\n",
                "ax.set_ylabel('F1-macro', fontsize=12)\n",
                "ax.set_title('Training Time vs Performance', fontsize=14, fontweight='bold')\n",
                "plt.colorbar(scatter, label='F1-macro')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'time_vs_performance.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n[OK] Training time chart saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Save best metrics\n",
                "with open(OUTPUT_DIR / 'metrics_label_spreading.json', 'w') as f:\n",
                "    json.dump(best_metrics, f, indent=2)\n",
                "\n",
                "# Save comparison\n",
                "comparison_df.to_csv(OUTPUT_DIR / 'method_comparison.csv', index=False)\n",
                "\n",
                "# Save summary\n",
                "summary = {\n",
                "    'best_config': label_spread_results[best_key]['params'],\n",
                "    'best_metrics': {\n",
                "        'accuracy': best_metrics['accuracy'],\n",
                "        'f1_macro': best_metrics['f1_macro']\n",
                "    },\n",
                "    'training_time': label_spread_results[best_key]['time'],\n",
                "    'sampled': best_metrics['sampled'],\n",
                "    'sample_size': best_metrics['sample_size'],\n",
                "    'all_configs': [\n",
                "        {\n",
                "            'config': data['params'],\n",
                "            'accuracy': data['results']['test_metrics']['accuracy'],\n",
                "            'f1_macro': data['results']['test_metrics']['f1_macro'],\n",
                "            'time': data['time']\n",
                "        }\n",
                "        for key, data in label_spread_results.items()\n",
                "    ]\n",
                "}\n",
                "\n",
                "with open(OUTPUT_DIR / 'label_spreading_summary.json', 'w') as f:\n",
                "    json.dump(summary, f, indent=2)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "print(json.dumps(summary, indent=2))\n",
                "print(\"\\n[OK] All results saved to:\", OUTPUT_DIR)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Analysis & Conclusion\n",
                "\n",
                "**Label Spreading Characteristics:**\n",
                "- Uses manifold structure → better for clustered data\n",
                "- Single optimization → faster than iterative methods\n",
                "- Memory intensive → requires sampling for large datasets\n",
                "\n",
                "**When to use Label Spreading:**\n",
                "- Data has clear manifold structure\n",
                "- Dataset size manageable (< 100K samples)\n",
                "- Want deterministic results (no randomness in pseudo-labeling)\n",
                "\n",
                "**When to use Self-Training/FlexMatch:**\n",
                "- Large datasets (> 100K samples)\n",
                "- Need iterative refinement\n",
                "- Want to control pseudo-label selection explicitly"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "beijing_env",
            "language": "python",
            "name": "beijing_env"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}