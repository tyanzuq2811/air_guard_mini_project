{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "558b5707",
   "metadata": {},
   "source": [
    "# Self-Training vá»›i Nhiá»u NgÆ°á»¡ng Ï„ - ThÃ­ Nghiá»‡m So SÃ¡nh\n",
    "\n",
    "## Má»¥c tiÃªu\n",
    "- Cháº¡y self-training vá»›i **nhiá»u giÃ¡ trá»‹ ngÆ°á»¡ng Ï„ khÃ¡c nhau** (0.70, 0.80, 0.85, 0.90, 0.95)\n",
    "- So sÃ¡nh diá»…n biáº¿n qua cÃ¡c vÃ²ng láº·p\n",
    "- PhÃ¢n tÃ­ch sá»‘ lÆ°á»£ng pseudo-labels Ä‘Æ°á»£c thÃªm má»—i vÃ²ng\n",
    "- Quan sÃ¡t validation accuracy/F1-macro qua cÃ¡c vÃ²ng\n",
    "- So sÃ¡nh káº¿t quáº£ test vá»›i baseline\n",
    "- Chá»n ngÆ°á»¡ng Ï„ tá»‘i Æ°u\n",
    "\n",
    "## Giáº£i thÃ­ch NgÆ°á»¡ng Ï„\n",
    "\n",
    "**NgÆ°á»¡ng Ï„ (tau)** lÃ  Ä‘á»™ tin cáº­y tá»‘i thiá»ƒu mÃ  mÃ´ hÃ¬nh cáº§n Ä‘áº¡t Ä‘Æ°á»£c khi dá»± Ä‘oÃ¡n.\n",
    "\n",
    "### VÃ­ dá»¥ Ä‘Æ¡n giáº£n:\n",
    "Giáº£ sá»­ báº¡n lÃ  giÃ¡o viÃªn cháº¥m bÃ i thi tráº¯c nghiá»‡m:\n",
    "- **Há»c sinh A** chá»n Ä‘Ã¡p Ã¡n B vÃ  nÃ³i: \"Em cháº¯c cháº¯n 95% lÃ  Ä‘Ã¡p Ã¡n B\"\n",
    "- **Há»c sinh B** chá»n Ä‘Ã¡p Ã¡n C vÃ  nÃ³i: \"Em Ä‘oÃ¡n thÃ´i, khoáº£ng 60% lÃ  Ä‘Ã¡p Ã¡n C\"\n",
    "\n",
    "Náº¿u báº¡n Ä‘áº·t ngÆ°á»¡ng Ï„ = 0.90:\n",
    "- âœ… Há»c sinh A (95% â‰¥ 90%) â†’ Báº N TIN vÃ  dÃ¹ng cÃ¢u tráº£ lá»i nÃ y\n",
    "- âŒ Há»c sinh B (60% < 90%) â†’ Báº N KHÃ”NG TIN vÃ  bá» qua\n",
    "\n",
    "TÆ°Æ¡ng tá»±, trong self-training:\n",
    "- MÃ´ hÃ¬nh dá»± Ä‘oÃ¡n nhiá»u máº«u chÆ°a cÃ³ nhÃ£n\n",
    "- Chá»‰ nhá»¯ng dá»± Ä‘oÃ¡n cÃ³ Ä‘á»™ tin cáº­y â‰¥ Ï„ má»›i Ä‘Æ°á»£c cháº¥p nháº­n lÃ m \"nhÃ£n giáº£\"\n",
    "- Nhá»¯ng nhÃ£n giáº£ nÃ y Ä‘Æ°á»£c thÃªm vÃ o táº­p train Ä‘á»ƒ train tiáº¿p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e50afd",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# PARAMETERS - CÃ³ thá»ƒ Ä‘iá»u chá»‰nh\n",
    "SEMI_DATASET_PATH = \"data/processed/dataset_for_semi.parquet\"\n",
    "CUTOFF = \"2017-01-01\"\n",
    "\n",
    "# Danh sÃ¡ch cÃ¡c giÃ¡ trá»‹ TAU Ä‘á»ƒ thá»­ nghiá»‡m\n",
    "TAU_VALUES = [0.70, 0.80, 0.85, 0.90, 0.95]\n",
    "\n",
    "# CÃ¡c tham sá»‘ cá»‘ Ä‘á»‹nh\n",
    "MAX_ITER = 10\n",
    "MIN_NEW_PER_ITER = 20\n",
    "VAL_FRAC = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Output paths\n",
    "RESULTS_DIR = \"data/processed/self_training_experiments\"\n",
    "ALERT_FROM_CLASS = \"Unhealthy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from src.semi_supervised_library import (\n",
    "    SemiDataConfig, SelfTrainingConfig, run_self_training, add_alert_columns\n",
    ")\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "if not (PROJECT_ROOT / \"data\").exists() and (PROJECT_ROOT.parent / \"data\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent.resolve()\n",
    "\n",
    "results_dir = (PROJECT_ROOT / RESULTS_DIR).resolve()\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Results directory: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc74c3",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 1: Load Dataset vÃ  Kiá»ƒm Tra Tá»‰ Lá»‡ Labeled/Unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_parquet((PROJECT_ROOT / SEMI_DATASET_PATH).resolve())\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "\n",
    "# Check labeled ratio\n",
    "if \"is_labeled\" in df.columns:\n",
    "    labeled_ratio = df[\"is_labeled\"].mean()\n",
    "    print(f\"\\nLabeled ratio: {labeled_ratio:.2%}\")\n",
    "    print(f\"Labeled samples: {df['is_labeled'].sum():,}\")\n",
    "    print(f\"Unlabeled samples: {(~df['is_labeled']).sum():,}\")\n",
    "else:\n",
    "    print(\"\\nWarning: 'is_labeled' column not found\")\n",
    "\n",
    "# Check train/test split\n",
    "train_mask = df[\"datetime\"] < pd.Timestamp(CUTOFF)\n",
    "test_mask = df[\"datetime\"] >= pd.Timestamp(CUTOFF)\n",
    "\n",
    "print(f\"\\nTrain samples: {train_mask.sum():,}\")\n",
    "print(f\"Test samples: {test_mask.sum():,}\")\n",
    "\n",
    "if \"is_labeled\" in df.columns:\n",
    "    print(f\"\\nTrain labeled: {(train_mask & df['is_labeled']).sum():,}\")\n",
    "    print(f\"Train unlabeled: {(train_mask & ~df['is_labeled']).sum():,}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9956e1f7",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 2: Cháº¡y Self-Training vá»›i Nhiá»u GiÃ¡ Trá»‹ Ï„\n",
    "\n",
    "### Giáº£i thÃ­ch quy trÃ¬nh:\n",
    "```\n",
    "Vá»›i má»—i giÃ¡ trá»‹ Ï„:\n",
    "  VÃ²ng 1:\n",
    "    1. Train model trÃªn labeled data (5%)\n",
    "    2. Dá»± Ä‘oÃ¡n trÃªn unlabeled data (95%)\n",
    "    3. Chá»n nhá»¯ng dá»± Ä‘oÃ¡n cÃ³ confidence â‰¥ Ï„\n",
    "    4. ThÃªm vÃ o labeled data\n",
    "  \n",
    "  VÃ²ng 2:\n",
    "    1. Train model trÃªn labeled data má»Ÿ rá»™ng\n",
    "    2. Dá»± Ä‘oÃ¡n trÃªn unlabeled cÃ²n láº¡i\n",
    "    3. Chá»n nhá»¯ng dá»± Ä‘oÃ¡n cÃ³ confidence â‰¥ Ï„\n",
    "    4. ThÃªm vÃ o labeled data\n",
    "  \n",
    "  ... láº·p láº¡i cho Ä‘áº¿n khi:\n",
    "    - KhÃ´ng cÃ²n Ä‘á»§ pseudo-labels confident\n",
    "    - Hoáº·c Ä‘áº¡t max_iter\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b40ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary Ä‘á»ƒ lÆ°u káº¿t quáº£ tá»«ng tau\n",
    "all_results = {}\n",
    "\n",
    "data_cfg = SemiDataConfig(cutoff=CUTOFF, random_state=RANDOM_STATE)\n",
    "\n",
    "for tau in TAU_VALUES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running Self-Training with TAU = {tau}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    st_cfg = SelfTrainingConfig(\n",
    "        tau=tau,\n",
    "        max_iter=MAX_ITER,\n",
    "        min_new_per_iter=MIN_NEW_PER_ITER,\n",
    "        val_frac=VAL_FRAC,\n",
    "    )\n",
    "    \n",
    "    # Run self-training\n",
    "    out = run_self_training(df.copy(), data_cfg, st_cfg)\n",
    "    \n",
    "    history = pd.DataFrame(out[\"history\"])\n",
    "    test_metrics = out[\"test_metrics\"]\n",
    "    pred_df = out[\"pred_df\"]\n",
    "    \n",
    "    # Display history\n",
    "    print(\"\\nTraining History:\")\n",
    "    display(history)\n",
    "    \n",
    "    print(f\"\\nTest Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Test F1-macro: {test_metrics['f1_macro']:.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    tau_str = str(tau).replace('.', '_')\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_path = results_dir / f\"metrics_tau_{tau_str}.json\"\n",
    "    payload = {\n",
    "        \"method\": \"self_training\",\n",
    "        \"tau\": tau,\n",
    "        \"data_cfg\": data_cfg.__dict__,\n",
    "        \"st_cfg\": st_cfg.__dict__,\n",
    "        \"history\": out[\"history\"],\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"model_info\": out[\"model_info\"],\n",
    "    }\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Save predictions\n",
    "    pred_path = results_dir / f\"predictions_tau_{tau_str}.csv\"\n",
    "    pred_df.to_csv(pred_path, index=False)\n",
    "    \n",
    "    # Save to memory for comparison\n",
    "    all_results[tau] = {\n",
    "        \"history\": history,\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"pred_df\": pred_df,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nâœ“ Saved: {metrics_path}\")\n",
    "    print(f\"âœ“ Saved: {pred_path}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All experiments completed!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b36bba",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 3: So SÃ¡nh Káº¿t Quáº£ Test vá»›i CÃ¡c GiÃ¡ Trá»‹ Ï„ KhÃ¡c Nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0cb317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Táº¡o báº£ng so sÃ¡nh\n",
    "comparison_data = []\n",
    "\n",
    "for tau in TAU_VALUES:\n",
    "    metrics = all_results[tau][\"test_metrics\"]\n",
    "    history = all_results[tau][\"history\"]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        \"TAU (Ï„)\": tau,\n",
    "        \"Test Accuracy\": metrics[\"accuracy\"],\n",
    "        \"Test F1-macro\": metrics[\"f1_macro\"],\n",
    "        \"Sá»‘ vÃ²ng láº·p\": len(history),\n",
    "        \"Tá»•ng pseudo-labels\": history[\"new_pseudo\"].sum(),\n",
    "        \"Val F1 cuá»‘i\": history[\"val_f1_macro\"].iloc[-1],\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Báº¢NG SO SÃNH Káº¾T QUáº¢ CÃC GIÃ TRá»Š Ï„\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_df)\n",
    "\n",
    "# Highlight best results\n",
    "best_acc_idx = comparison_df[\"Test Accuracy\"].idxmax()\n",
    "best_f1_idx = comparison_df[\"Test F1-macro\"].idxmax()\n",
    "\n",
    "print(\"\\nğŸ“Š Káº¾T QUáº¢ Tá»T NHáº¤T:\")\n",
    "print(f\"- Accuracy cao nháº¥t: Ï„ = {comparison_df.loc[best_acc_idx, 'TAU (Ï„)']} â†’ {comparison_df.loc[best_acc_idx, 'Test Accuracy']:.4f}\")\n",
    "print(f\"- F1-macro cao nháº¥t: Ï„ = {comparison_df.loc[best_f1_idx, 'TAU (Ï„)']} â†’ {comparison_df.loc[best_f1_idx, 'Test F1-macro']:.4f}\")\n",
    "\n",
    "# Save comparison\n",
    "comparison_path = results_dir / \"comparison_summary.csv\"\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "print(f\"\\nâœ“ Saved: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d883a1f",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 4: Visualize - So SÃ¡nh Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d71e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test performance comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].bar(comparison_df[\"TAU (Ï„)\"].astype(str), comparison_df[\"Test Accuracy\"], color='steelblue', alpha=0.7)\n",
    "axes[0].set_xlabel(\"NgÆ°á»¡ng Ï„\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Test Accuracy\", fontsize=12)\n",
    "axes[0].set_title(\"So SÃ¡nh Test Accuracy theo Ï„\", fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(comparison_df[\"Test Accuracy\"]):\n",
    "    axes[0].text(i, v + 0.005, f\"{v:.4f}\", ha='center', fontsize=9)\n",
    "\n",
    "# F1-macro\n",
    "axes[1].bar(comparison_df[\"TAU (Ï„)\"].astype(str), comparison_df[\"Test F1-macro\"], color='coral', alpha=0.7)\n",
    "axes[1].set_xlabel(\"NgÆ°á»¡ng Ï„\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Test F1-macro\", fontsize=12)\n",
    "axes[1].set_title(\"So SÃ¡nh Test F1-macro theo Ï„\", fontsize=13, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(comparison_df[\"Test F1-macro\"]):\n",
    "    axes[1].text(i, v + 0.005, f\"{v:.4f}\", ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / \"test_performance_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Saved: {results_dir / 'test_performance_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca9b20d",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 5: Visualize - Diá»…n Biáº¿n Sá»‘ Pseudo-Labels Qua CÃ¡c VÃ²ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ffeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pseudo-labels over iterations\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for tau in TAU_VALUES:\n",
    "    history = all_results[tau][\"history\"]\n",
    "    ax.plot(history[\"iter\"], history[\"new_pseudo\"], marker='o', label=f\"Ï„ = {tau}\", linewidth=2)\n",
    "\n",
    "ax.set_xlabel(\"VÃ²ng láº·p (Iteration)\", fontsize=12)\n",
    "ax.set_ylabel(\"Sá»‘ pseudo-labels thÃªm vÃ o\", fontsize=12)\n",
    "ax.set_title(\"Diá»…n Biáº¿n Sá»‘ Pseudo-Labels Qua CÃ¡c VÃ²ng\\n(So sÃ¡nh cÃ¡c giÃ¡ trá»‹ Ï„ khÃ¡c nhau)\", \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(range(1, MAX_ITER + 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / \"pseudo_labels_over_iterations.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Saved: {results_dir / 'pseudo_labels_over_iterations.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d843f",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 6: Visualize - Validation F1-macro Qua CÃ¡c VÃ²ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6e25be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation F1-macro over iterations\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for tau in TAU_VALUES:\n",
    "    history = all_results[tau][\"history\"]\n",
    "    ax.plot(history[\"iter\"], history[\"val_f1_macro\"], marker='s', label=f\"Ï„ = {tau}\", linewidth=2)\n",
    "\n",
    "ax.set_xlabel(\"VÃ²ng láº·p (Iteration)\", fontsize=12)\n",
    "ax.set_ylabel(\"Validation F1-macro\", fontsize=12)\n",
    "ax.set_title(\"Diá»…n Biáº¿n Validation F1-macro Qua CÃ¡c VÃ²ng\\n(So sÃ¡nh cÃ¡c giÃ¡ trá»‹ Ï„ khÃ¡c nhau)\", \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(range(1, MAX_ITER + 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / \"validation_f1_over_iterations.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Saved: {results_dir / 'validation_f1_over_iterations.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c036b4b7",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 7: PhÃ¢n TÃ­ch Chi Tiáº¿t Tá»«ng Ï„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f8762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tau in TAU_VALUES:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PHÃ‚N TÃCH CHI TIáº¾T: TAU = {tau}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    history = all_results[tau][\"history\"]\n",
    "    metrics = all_results[tau][\"test_metrics\"]\n",
    "    \n",
    "    print(\"\\nğŸ“Š Diá»…n biáº¿n qua cÃ¡c vÃ²ng:\")\n",
    "    display(history)\n",
    "    \n",
    "    # Observations\n",
    "    print(\"\\nğŸ” NHáº¬N XÃ‰T:\")\n",
    "    \n",
    "    # 1. VÃ²ng Ä‘áº§u thÃªm bao nhiÃªu?\n",
    "    first_iter_pseudo = history[\"new_pseudo\"].iloc[0]\n",
    "    print(f\"\\n1. VÃ²ng Ä‘áº§u tiÃªn:\")\n",
    "    print(f\"   - ThÃªm Ä‘Æ°á»£c {first_iter_pseudo} pseudo-labels\")\n",
    "    if first_iter_pseudo > 1000:\n",
    "        print(f\"   âš ï¸ Sá»‘ lÆ°á»£ng Ráº¤T NHIá»€U â†’ Model quÃ¡ tá»± tin hoáº·c gáº·p nhiá»u máº«u dá»…\")\n",
    "    elif first_iter_pseudo > 500:\n",
    "        print(f\"   âœ“ Sá»‘ lÆ°á»£ng Vá»ªA PHáº¢I â†’ Model cÃ³ Ä‘á»™ tin cáº­y tá»‘t\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ Sá»‘ lÆ°á»£ng ÃT â†’ Model tháº­n trá»ng quÃ¡ hoáº·c Ï„ quÃ¡ cao\")\n",
    "    \n",
    "    # 2. Xu hÆ°á»›ng tÄƒng/giáº£m\n",
    "    print(f\"\\n2. Xu hÆ°á»›ng sá»‘ pseudo-labels:\")\n",
    "    if len(history) > 2:\n",
    "        trend = history[\"new_pseudo\"].diff().iloc[1:]\n",
    "        if (trend < 0).all():\n",
    "            print(f\"   â†“ GIáº¢M Dáº¦N qua cÃ¡c vÃ²ng â†’ BÃ¬nh thÆ°á»ng (háº¿t máº«u dá»…)\")\n",
    "        elif (trend > 0).any():\n",
    "            print(f\"   â†‘ CÃ“ VÃ’NG TÄ‚NG â†’ Model há»c tá»‘t hÆ¡n, tá»± tin hÆ¡n\")\n",
    "        else:\n",
    "            print(f\"   â†’ á»”N Äá»ŠNH\")\n",
    "    \n",
    "    # 3. Validation F1 cÃ³ giáº£m khÃ´ng?\n",
    "    print(f\"\\n3. Validation F1-macro:\")\n",
    "    val_f1_diff = history[\"val_f1_macro\"].diff()\n",
    "    decrease_iters = history[val_f1_diff < -0.01][\"iter\"].tolist()\n",
    "    \n",
    "    if decrease_iters:\n",
    "        print(f\"   âš ï¸ GIáº¢M á»Ÿ vÃ²ng: {decrease_iters}\")\n",
    "        print(f\"   â†’ NguyÃªn nhÃ¢n: Model cÃ³ thá»ƒ Ä‘Ã£ thÃªm nhÃ£n SAI vÃ  há»c theo chÃºng\")\n",
    "        print(f\"   â†’ CÃ¢n nháº¯c Dá»ªNG Sá»šM á»Ÿ vÃ²ng {decrease_iters[0] - 1}\")\n",
    "    else:\n",
    "        print(f\"   âœ“ TÄ‚NG hoáº·c á»”N Äá»ŠNH qua cÃ¡c vÃ²ng\")\n",
    "        print(f\"   â†’ Model há»c tá»‘t, khÃ´ng bá»‹ overfitting\")\n",
    "    \n",
    "    # 4. Test performance\n",
    "    print(f\"\\n4. Káº¿t quáº£ trÃªn Test set:\")\n",
    "    print(f\"   - Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"   - Test F1-macro: {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"   - Sá»‘ vÃ²ng láº·p: {len(history)}\")\n",
    "    print(f\"   - Tá»•ng pseudo-labels: {history['new_pseudo'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aafbd1d",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 8: Load Baseline vÃ  So SÃ¡nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37917176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline metrics\n",
    "baseline_path = PROJECT_ROOT / \"data/processed/metrics.json\"\n",
    "\n",
    "if baseline_path.exists():\n",
    "    with open(baseline_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        baseline_metrics = json.load(f)\n",
    "    \n",
    "    baseline_acc = baseline_metrics.get(\"accuracy\", None)\n",
    "    baseline_f1 = baseline_metrics.get(\"f1_macro\", None)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SO SÃNH Vá»šI BASELINE SUPERVISED (100% labels)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nBaseline (Supervised vá»›i 100% labels):\")\n",
    "    print(f\"  - Accuracy: {baseline_acc:.4f}\")\n",
    "    print(f\"  - F1-macro: {baseline_f1:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Self-Training Results (chá»‰ dÃ¹ng 5% labels ban Ä‘áº§u):\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for tau in TAU_VALUES:\n",
    "        metrics = all_results[tau][\"test_metrics\"]\n",
    "        acc = metrics[\"accuracy\"]\n",
    "        f1 = metrics[\"f1_macro\"]\n",
    "        \n",
    "        acc_diff = acc - baseline_acc\n",
    "        f1_diff = f1 - baseline_f1\n",
    "        \n",
    "        print(f\"\\nÏ„ = {tau}:\")\n",
    "        print(f\"  - Accuracy: {acc:.4f} ({acc_diff:+.4f}) {'âœ“' if acc_diff >= 0 else 'â†“'}\")\n",
    "        print(f\"  - F1-macro: {f1:.4f} ({f1_diff:+.4f}) {'âœ“' if f1_diff >= 0 else 'â†“'}\")\n",
    "        \n",
    "        if acc_diff >= -0.02:\n",
    "            print(f\"  ğŸ’¡ Self-training Äáº T Gáº¦N baseline máº·c dÃ¹ chá»‰ dÃ¹ng 5% labels!\")\n",
    "        elif acc_diff < -0.05:\n",
    "            print(f\"  âš ï¸ ChÃªnh lá»‡ch lá»›n â†’ Cáº§n Ä‘iá»u chá»‰nh Ï„ hoáº·c tÄƒng labeled data\")\n",
    "    \n",
    "    # Plot comparison with baseline\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    x_pos = np.arange(len(TAU_VALUES) + 1)\n",
    "    acc_values = [baseline_acc] + [all_results[tau][\"test_metrics\"][\"accuracy\"] for tau in TAU_VALUES]\n",
    "    labels = [\"Baseline\\n(100%)\"  ] + [f\"Ï„={tau}\\n(5%)\" for tau in TAU_VALUES]\n",
    "    colors = ['gold'] + ['steelblue'] * len(TAU_VALUES)\n",
    "    \n",
    "    axes[0].bar(x_pos, acc_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_xticks(x_pos)\n",
    "    axes[0].set_xticklabels(labels)\n",
    "    axes[0].set_ylabel(\"Test Accuracy\", fontsize=12)\n",
    "    axes[0].set_title(\"So SÃ¡nh Accuracy: Baseline vs Self-Training\", fontsize=13, fontweight='bold')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    axes[0].axhline(y=baseline_acc, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Baseline')\n",
    "    for i, v in enumerate(acc_values):\n",
    "        axes[0].text(i, v + 0.005, f\"{v:.4f}\", ha='center', fontsize=9)\n",
    "    \n",
    "    # F1-macro comparison\n",
    "    f1_values = [baseline_f1] + [all_results[tau][\"test_metrics\"][\"f1_macro\"] for tau in TAU_VALUES]\n",
    "    \n",
    "    axes[1].bar(x_pos, f1_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xticks(x_pos)\n",
    "    axes[1].set_xticklabels(labels)\n",
    "    axes[1].set_ylabel(\"Test F1-macro\", fontsize=12)\n",
    "    axes[1].set_title(\"So SÃ¡nh F1-macro: Baseline vs Self-Training\", fontsize=13, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    axes[1].axhline(y=baseline_f1, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Baseline')\n",
    "    for i, v in enumerate(f1_values):\n",
    "        axes[1].text(i, v + 0.005, f\"{v:.4f}\", ha='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / \"comparison_with_baseline.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nâœ“ Saved: {results_dir / 'comparison_with_baseline.png'}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nâš ï¸ Baseline metrics not found at: {baseline_path}\")\n",
    "    print(\"Run classification_modelling.ipynb first to generate baseline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72503e7e",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 9: PhÃ¢n TÃ­ch Per-Class Performance (Lá»›p nÃ o Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076d7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chá»n tau tá»‘t nháº¥t Ä‘á»ƒ phÃ¢n tÃ­ch chi tiáº¿t\n",
    "best_tau = comparison_df.loc[comparison_df[\"Test F1-macro\"].idxmax(), \"TAU (Ï„)\"]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PHÃ‚N TÃCH CHI TIáº¾T THEO Lá»šP - TAU Tá»I Æ¯U: {best_tau}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "metrics = all_results[best_tau][\"test_metrics\"]\n",
    "report = metrics[\"report\"]\n",
    "\n",
    "# Extract per-class metrics\n",
    "class_metrics = []\n",
    "for class_name in metrics[\"labels\"]:\n",
    "    if class_name in report:\n",
    "        class_metrics.append({\n",
    "            \"Class\": class_name,\n",
    "            \"Precision\": report[class_name][\"precision\"],\n",
    "            \"Recall\": report[class_name][\"recall\"],\n",
    "            \"F1-score\": report[class_name][\"f1-score\"],\n",
    "            \"Support\": report[class_name][\"support\"],\n",
    "        })\n",
    "\n",
    "class_df = pd.DataFrame(class_metrics)\n",
    "display(class_df)\n",
    "\n",
    "# Compare with baseline if available\n",
    "if baseline_path.exists():\n",
    "    baseline_report = baseline_metrics.get(\"report\", {})\n",
    "    \n",
    "    if baseline_report:\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"SO SÃNH Vá»šI BASELINE THEO Tá»ªNG Lá»šP:\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        comparison_class = []\n",
    "        for class_name in metrics[\"labels\"]:\n",
    "            if class_name in report and class_name in baseline_report:\n",
    "                st_f1 = report[class_name][\"f1-score\"]\n",
    "                bl_f1 = baseline_report[class_name][\"f1-score\"]\n",
    "                diff = st_f1 - bl_f1\n",
    "                \n",
    "                comparison_class.append({\n",
    "                    \"Class\": class_name,\n",
    "                    \"Baseline F1\": bl_f1,\n",
    "                    \"Self-Train F1\": st_f1,\n",
    "                    \"ChÃªnh lá»‡ch\": diff,\n",
    "                    \"Tráº¡ng thÃ¡i\": \"âœ“ Cáº£i thiá»‡n\" if diff > 0.01 else (\"â†“ Giáº£m\" if diff < -0.01 else \"â‰ˆ TÆ°Æ¡ng Ä‘Æ°Æ¡ng\"),\n",
    "                })\n",
    "        \n",
    "        comp_class_df = pd.DataFrame(comparison_class)\n",
    "        display(comp_class_df)\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        x = np.arange(len(comp_class_df))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax.bar(x - width/2, comp_class_df[\"Baseline F1\"], width, label='Baseline', alpha=0.7, color='gold')\n",
    "        ax.bar(x + width/2, comp_class_df[\"Self-Train F1\"], width, label=f'Self-Training (Ï„={best_tau})', alpha=0.7, color='steelblue')\n",
    "        \n",
    "        ax.set_xlabel(\"AQI Class\", fontsize=12)\n",
    "        ax.set_ylabel(\"F1-score\", fontsize=12)\n",
    "        ax.set_title(f\"So SÃ¡nh F1-score Theo Tá»«ng Lá»›p\\nBaseline vs Self-Training (Ï„={best_tau})\", fontsize=13, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(comp_class_df[\"Class\"], rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(results_dir / \"per_class_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nâœ“ Saved: {results_dir / 'per_class_comparison.png'}\")\n",
    "        \n",
    "        # Identify benefited classes\n",
    "        print(\"\\nğŸ¯ NHáº¬N XÃ‰T:\")\n",
    "        improved = comp_class_df[comp_class_df[\"ChÃªnh lá»‡ch\"] > 0.01]\n",
    "        degraded = comp_class_df[comp_class_df[\"ChÃªnh lá»‡ch\"] < -0.01]\n",
    "        \n",
    "        if not improved.empty:\n",
    "            print(f\"\\nâœ“ CÃ¡c lá»›p ÄÆ¯á»¢C Cáº¢I THIá»†N:\")\n",
    "            for _, row in improved.iterrows():\n",
    "                print(f\"  - {row['Class']}: +{row['ChÃªnh lá»‡ch']:.4f}\")\n",
    "        \n",
    "        if not degraded.empty:\n",
    "            print(f\"\\nâ†“ CÃ¡c lá»›p Bá»Š GIáº¢M:\")\n",
    "            for _, row in degraded.iterrows():\n",
    "                print(f\"  - {row['Class']}: {row['ChÃªnh lá»‡ch']:.4f}\")\n",
    "\n",
    "# Save final comparison\n",
    "final_comparison_path = results_dir / \"per_class_comparison.csv\"\n",
    "if 'comp_class_df' in locals():\n",
    "    comp_class_df.to_csv(final_comparison_path, index=False)\n",
    "    print(f\"\\nâœ“ Saved: {final_comparison_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89f7e74",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 10: Káº¿t Luáº­n vÃ  Khuyáº¿n Nghá»‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16359aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Káº¾T LUáº¬N VÃ€ KHUYáº¾N NGHá»Š\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_f1_tau = comparison_df.loc[comparison_df[\"Test F1-macro\"].idxmax(), \"TAU (Ï„)\"]\n",
    "best_f1_value = comparison_df[\"Test F1-macro\"].max()\n",
    "\n",
    "print(f\"\\n1ï¸âƒ£ NGÆ¯á» NG Ï„ Tá»I Æ¯U: {best_f1_tau}\")\n",
    "print(f\"   - Äáº¡t Test F1-macro: {best_f1_value:.4f}\")\n",
    "print(f\"   - Sá»‘ vÃ²ng láº·p: {comparison_df[comparison_df['TAU (Ï„)'] == best_f1_tau]['Sá»‘ vÃ²ng láº·p'].values[0]}\")\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ SO SÃNH Vá»šI BASELINE:\")\n",
    "if baseline_path.exists():\n",
    "    diff_from_baseline = best_f1_value - baseline_f1\n",
    "    if diff_from_baseline >= -0.02:\n",
    "        print(f\"   âœ… Self-training Äáº T THÃ€NH CÃ”NG!\")\n",
    "        print(f\"   - Chá»‰ dÃ¹ng 5% labels nhÆ°ng Ä‘áº¡t {best_f1_value:.4f}\")\n",
    "        print(f\"   - So vá»›i baseline (100% labels): {diff_from_baseline:+.4f}\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ CÃ²n khoáº£ng cÃ¡ch vá»›i baseline: {diff_from_baseline:.4f}\")\n",
    "        print(f\"   - Cáº§n xem xÃ©t tÄƒng labeled data hoáº·c Ä‘iá»u chá»‰nh tham sá»‘\")\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£ Táº¤T Cáº¢ Káº¾T QUáº¢ ÄÃƒ LÆ¯U Táº I:\")\n",
    "print(f\"   ğŸ“ {results_dir}\")\n",
    "print(f\"   - Metrics JSON cho má»—i Ï„\")\n",
    "print(f\"   - Predictions CSV\")\n",
    "print(f\"   - Biá»ƒu Ä‘á»“ so sÃ¡nh\")\n",
    "print(f\"   - Báº£ng tá»•ng há»£p\")\n",
    "\n",
    "print(f\"\\n4ï¸âƒ£ BÆ¯á»šC TIáº¾P THEO:\")\n",
    "print(f\"   â†’ Cháº¡y Co-Training vá»›i Ï„ = {best_f1_tau}\")\n",
    "print(f\"   â†’ So sÃ¡nh Self-Training vs Co-Training\")\n",
    "print(f\"   â†’ Viáº¿t bÃ¡o cÃ¡o chi tiáº¿t vá»›i cÃ¡c biá»ƒu Ä‘á»“ Ä‘Ã£ táº¡o\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}