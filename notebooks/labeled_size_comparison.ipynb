{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aab4613",
   "metadata": {},
   "source": [
    "# So S√°nh K√≠ch Th∆∞·ªõc T·∫≠p Labeled - Labeled Size Comparison (OPTIMIZED)\n",
    "\n",
    "## M·ª•c Ti√™u\n",
    "- Th·ª≠ nghi·ªám self-training v·ªõi **3 k√≠ch th∆∞·ªõc labeled data**: 5%, 10%, 20%\n",
    "- So s√°nh hi·ªáu qu·∫£ c·ªßa self-training khi c√≥ √≠t vs nhi·ªÅu labeled data\n",
    "- Tr·∫£ l·ªùi c√¢u h·ªèi: **Khi n√†o self-training c√≤n hi·ªáu qu·∫£?**\n",
    "- T√¨m ƒëi·ªÉm **diminishing return** (th√™m labeled data kh√¥ng c√≤n c·∫£i thi·ªán nhi·ªÅu)\n",
    "\n",
    "## Optimization Note\n",
    "‚ö° **B·ªè 1% labeled** (qu√° √≠t, model qu√° y·∫øu, m·∫•t ~5-7 ph√∫t)  \n",
    "‚úÖ **Gi·ªØ 5%, 10%, 20%** - 3 m·ª©c c√≥ contrast r√µ r√†ng nh·∫•t\n",
    "\n",
    "## Gi·∫£ Thuy·∫øt\n",
    "- **5% labeled** (baseline): Sweet spot cho semi-supervised\n",
    "- **10% labeled**: Model m·∫°nh h∆°n ‚Üí Self-training c·∫£i thi·ªán √≠t h∆°n\n",
    "- **20% labeled**: G·∫ßn supervised ‚Üí Self-training kh√¥ng c·∫ßn thi·∫øt\n",
    "\n",
    "## Setup\n",
    "- Ng∆∞·ª°ng œÑ = 0.90 (c·ªë ƒë·ªãnh)\n",
    "- Max iterations = 10\n",
    "- So s√°nh: F1-macro improvement vs supervised baseline\n",
    "- **Ti·∫øt ki·ªám: ~25% th·ªùi gian** (t·ª´ 20 ph√∫t ‚Üí 15 ph√∫t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "SEMI_DATASET_PATH = \"data/processed/dataset_for_semi.parquet\"\n",
    "CUTOFF = \"2017-01-01\"\n",
    "\n",
    "# Labeled fractions to compare (optimized: 3 configs for clear contrast)\n",
    "# B·ªè 1% (qu√° √≠t, model qu√° y·∫øu) - Gi·ªØ 5% (baseline), 10% (moderate), 20% (high)\n",
    "LABELED_FRACTIONS = [0.05, 0.10, 0.20]  # 5%, 10%, 20%\n",
    "\n",
    "# Fixed parameters\n",
    "TAU = 0.90\n",
    "MAX_ITER = 10\n",
    "MIN_NEW_PER_ITER = 20\n",
    "VAL_FRAC = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Output directory\n",
    "RESULTS_DIR = \"data/processed/labeled_size_experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f8ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from src.semi_supervised_library import (\n",
    "    SemiDataConfig, SelfTrainingConfig, run_self_training,\n",
    "    mask_labels_time_aware, time_split, build_feature_columns\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "if not (PROJECT_ROOT / \"data\").exists() and (PROJECT_ROOT.parent / \"data\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent.resolve()\n",
    "\n",
    "results_dir = (PROJECT_ROOT / RESULTS_DIR).resolve()\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Results directory: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854e548",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f43aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original dataset\n",
    "df_original = pd.read_parquet((PROJECT_ROOT / SEMI_DATASET_PATH).resolve())\n",
    "\n",
    "print(\"Original dataset shape:\", df_original.shape)\n",
    "print(\"Original labeled fraction:\", df_original['is_labeled'].mean())\n",
    "print(\"\\nColumns:\", df_original.columns.tolist()[:10], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8220f824",
   "metadata": {},
   "source": [
    "## Run Experiments for Each Labeled Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e2abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for frac in LABELED_FRACTIONS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EXPERIMENT: Labeled Fraction = {frac*100:.0f}% (œÑ={TAU})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create dataset with specific labeled fraction\n",
    "    # Re-mask labels to get desired fraction\n",
    "    df = df_original.copy()\n",
    "    \n",
    "    # Calculate missing fraction (inverse of labeled fraction)\n",
    "    missing_frac = 1.0 - frac\n",
    "    \n",
    "    # Create config and re-mask\n",
    "    data_cfg = SemiDataConfig(\n",
    "        cutoff=CUTOFF,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    df = mask_labels_time_aware(df, data_cfg, missing_fraction=missing_frac)\n",
    "    \n",
    "    train_df, test_df = time_split(df, cutoff=CUTOFF)\n",
    "    labeled_count = train_df['is_labeled'].sum()\n",
    "    unlabeled_count = (~train_df['is_labeled']).sum()\n",
    "    \n",
    "    print(f\"Training set:\")\n",
    "    print(f\"  - Labeled: {labeled_count:,} ({labeled_count/len(train_df)*100:.1f}%)\")\n",
    "    print(f\"  - Unlabeled: {unlabeled_count:,} ({unlabeled_count/len(train_df)*100:.1f}%)\")\n",
    "    print(f\"Test set: {len(test_df):,} samples\")\n",
    "    \n",
    "    # Run self-training\n",
    "    st_cfg = SelfTrainingConfig(\n",
    "        tau=TAU,\n",
    "        max_iter=MAX_ITER,\n",
    "        min_new_per_iter=MIN_NEW_PER_ITER,\n",
    "        val_frac=VAL_FRAC\n",
    "    )\n",
    "    \n",
    "    result = run_self_training(df, data_cfg, st_cfg)\n",
    "    \n",
    "    # Store results\n",
    "    results[f\"{frac*100:.0f}%\"] = {\n",
    "        \"labeled_fraction\": frac,\n",
    "        \"labeled_count\": int(labeled_count),\n",
    "        \"unlabeled_count\": int(unlabeled_count),\n",
    "        \"test_accuracy\": result[\"test_metrics\"][\"accuracy\"],\n",
    "        \"test_f1_macro\": result[\"test_metrics\"][\"f1_macro\"],\n",
    "        \"history\": result[\"history\"],\n",
    "        \"per_class_report\": result[\"test_metrics\"][\"report\"],\n",
    "        \"total_pseudo_labels\": sum([h[\"new_pseudo\"] for h in result[\"history\"]]),\n",
    "        \"iterations_completed\": len(result[\"history\"])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Completed {frac*100:.0f}%:\")\n",
    "    print(f\"   Test Accuracy: {result['test_metrics']['accuracy']:.4f}\")\n",
    "    print(f\"   Test F1-macro: {result['test_metrics']['f1_macro']:.4f}\")\n",
    "    print(f\"   Total pseudo-labels: {results[f'{frac*100:.0f}%']['total_pseudo_labels']:,}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL EXPERIMENTS COMPLETED\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9914a",
   "metadata": {},
   "source": [
    "## Save Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ef6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "output_file = results_dir / \"labeled_size_results.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved results to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db432ab3",
   "metadata": {},
   "source": [
    "## Create Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab21ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary dataframe\n",
    "summary_data = []\n",
    "for key, res in results.items():\n",
    "    summary_data.append({\n",
    "        \"Labeled %\": key,\n",
    "        \"Labeled Count\": res[\"labeled_count\"],\n",
    "        \"Unlabeled Count\": res[\"unlabeled_count\"],\n",
    "        \"Test Accuracy\": res[\"test_accuracy\"],\n",
    "        \"Test F1-macro\": res[\"test_f1_macro\"],\n",
    "        \"Pseudo-labels Added\": res[\"total_pseudo_labels\"],\n",
    "        \"Iterations\": res[\"iterations_completed\"]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values(\"Labeled Count\")\n",
    "\n",
    "print(\"\\nüìä SUMMARY TABLE:\")\n",
    "print(\"=\"*100)\n",
    "display(summary_df)\n",
    "\n",
    "# Save summary CSV\n",
    "summary_csv = results_dir / \"labeled_size_summary.csv\"\n",
    "summary_df.to_csv(summary_csv, index=False)\n",
    "print(f\"\\n‚úÖ Saved summary to: {summary_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6942500f",
   "metadata": {},
   "source": [
    "## Visualization 1: Test Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5acf2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy\n",
    "ax1 = axes[0]\n",
    "x_labels = summary_df[\"Labeled %\"].tolist()\n",
    "accuracies = summary_df[\"Test Accuracy\"].tolist()\n",
    "f1_scores = summary_df[\"Test F1-macro\"].tolist()\n",
    "\n",
    "bars1 = ax1.bar(x_labels, accuracies, color='skyblue', alpha=0.8, edgecolor='navy')\n",
    "ax1.set_xlabel(\"Labeled Data Fraction\", fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel(\"Test Accuracy\", fontsize=12, fontweight='bold')\n",
    "ax1.set_title(\"Test Accuracy vs Labeled Data Size\", fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim([0.5, 0.65])\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.4f}',\n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Plot 2: F1-macro\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(x_labels, f1_scores, color='coral', alpha=0.8, edgecolor='darkred')\n",
    "ax2.set_xlabel(\"Labeled Data Fraction\", fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel(\"Test F1-macro\", fontsize=12, fontweight='bold')\n",
    "ax2.set_title(\"Test F1-macro vs Labeled Data Size\", fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_ylim([0.4, 0.6])\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.4f}',\n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = results_dir / \"test_performance_comparison.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved plot: {plot_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9191ed4",
   "metadata": {},
   "source": [
    "## Visualization 2: Improvement over Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement (assuming 5% is our baseline)\n",
    "baseline_idx = summary_df[summary_df[\"Labeled %\"] == \"5%\"].index[0]\n",
    "baseline_f1 = summary_df.loc[baseline_idx, \"Test F1-macro\"]\n",
    "\n",
    "summary_df[\"F1 Improvement\"] = ((summary_df[\"Test F1-macro\"] - baseline_f1) / baseline_f1 * 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = ['red' if x < 0 else 'green' for x in summary_df[\"F1 Improvement\"]]\n",
    "bars = ax.barh(summary_df[\"Labeled %\"], summary_df[\"F1 Improvement\"], color=colors, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel(\"F1-macro Improvement vs 5% Baseline (%)\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"Labeled Data Fraction\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Relative Improvement Compared to 5% Labeled Baseline\", fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    label_x = width + (1 if width > 0 else -1)\n",
    "    ax.text(label_x, bar.get_y() + bar.get_height()/2.,\n",
    "            f'{width:.1f}%',\n",
    "            ha='left' if width > 0 else 'right',\n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = results_dir / \"improvement_vs_baseline.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved plot: {plot_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c734eb33",
   "metadata": {},
   "source": [
    "## Visualization 3: Pseudo-labels Added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "pseudo_counts = summary_df[\"Pseudo-labels Added\"].tolist()\n",
    "labeled_pcts = summary_df[\"Labeled %\"].tolist()\n",
    "\n",
    "bars = ax.bar(labeled_pcts, pseudo_counts, color='mediumpurple', alpha=0.8, edgecolor='indigo')\n",
    "ax.set_xlabel(\"Labeled Data Fraction\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"Total Pseudo-labels Added\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Pseudo-labeling Activity vs Labeled Data Size\", fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height):,}',\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = results_dir / \"pseudo_labels_comparison.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved plot: {plot_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed89e606",
   "metadata": {},
   "source": [
    "## Visualization 4: Training Data Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240f6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(labeled_pcts))\n",
    "width = 0.35\n",
    "\n",
    "labeled_counts = summary_df[\"Labeled Count\"].tolist()\n",
    "pseudo_counts = summary_df[\"Pseudo-labels Added\"].tolist()\n",
    "\n",
    "bars1 = ax.bar(x - width/2, labeled_counts, width, label='Initial Labeled', color='steelblue', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, pseudo_counts, width, label='Pseudo-labels Added', color='orange', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"Labeled Data Fraction\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"Sample Count\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Training Data Composition\", fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labeled_pcts)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height):,}',\n",
    "                ha='center', va='bottom', fontsize=9, rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = results_dir / \"training_data_composition.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved plot: {plot_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9be951",
   "metadata": {},
   "source": [
    "## Visualization 5: Learning Curves (F1 over Iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07fae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for key, res in results.items():\n",
    "    history = res[\"history\"]\n",
    "    iterations = [h[\"iter\"] for h in history]\n",
    "    val_f1 = [h[\"val_f1_macro\"] for h in history]\n",
    "    \n",
    "    ax.plot(iterations, val_f1, marker='o', linewidth=2, label=f'{key} labeled', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"Iteration\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"Validation F1-macro\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Validation Learning Curves for Different Labeled Sizes\", fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='best')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xticks(range(1, 11))\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = results_dir / \"learning_curves.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved plot: {plot_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd01ce4",
   "metadata": {},
   "source": [
    "## Analysis & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd8627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä KEY FINDINGS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Find best configuration\n",
    "best_idx = summary_df[\"Test F1-macro\"].idxmax()\n",
    "best_config = summary_df.loc[best_idx]\n",
    "\n",
    "print(f\"\\nüèÜ Best Configuration:\")\n",
    "print(f\"   Labeled Fraction: {best_config['Labeled %']}\")\n",
    "print(f\"   Test F1-macro: {best_config['Test F1-macro']:.4f}\")\n",
    "print(f\"   Test Accuracy: {best_config['Test Accuracy']:.4f}\")\n",
    "print(f\"   Pseudo-labels: {best_config['Pseudo-labels Added']:,}\")\n",
    "\n",
    "# Efficiency analysis\n",
    "print(f\"\\nüí° Efficiency Analysis:\")\n",
    "for idx, row in summary_df.iterrows():\n",
    "    efficiency = row[\"Test F1-macro\"] / (row[\"Labeled Count\"] / 1000)\n",
    "    print(f\"   {row['Labeled %']}: {efficiency:.4f} F1 per 1K labeled samples\")\n",
    "\n",
    "# Diminishing returns\n",
    "print(f\"\\nüìâ Diminishing Returns:\")\n",
    "for i in range(1, len(summary_df)):\n",
    "    prev_f1 = summary_df.iloc[i-1][\"Test F1-macro\"]\n",
    "    curr_f1 = summary_df.iloc[i][\"Test F1-macro\"]\n",
    "    improvement = (curr_f1 - prev_f1) * 100\n",
    "    prev_label = summary_df.iloc[i-1][\"Labeled %\"]\n",
    "    curr_label = summary_df.iloc[i][\"Labeled %\"]\n",
    "    print(f\"   {prev_label} ‚Üí {curr_label}: +{improvement:.2f}% F1 improvement\")\n",
    "\n",
    "print(f\"\\n‚úÖ All visualizations saved to: {results_dir}\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a31f659",
   "metadata": {},
   "source": [
    "## Summary for Dashboard\n",
    "\n",
    "Export simplified data for Streamlit dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dashboard-friendly summary\n",
    "dashboard_data = {\n",
    "    \"experiment_type\": \"labeled_size_comparison\",\n",
    "    \"parameters\": {\n",
    "        \"tau\": TAU,\n",
    "        \"max_iter\": MAX_ITER,\n",
    "        \"labeled_fractions\": LABELED_FRACTIONS\n",
    "    },\n",
    "    \"summary\": summary_df.to_dict(orient='records'),\n",
    "    \"best_config\": {\n",
    "        \"labeled_fraction\": best_config[\"Labeled %\"],\n",
    "        \"f1_macro\": float(best_config[\"Test F1-macro\"]),\n",
    "        \"accuracy\": float(best_config[\"Test Accuracy\"])\n",
    "    },\n",
    "    \"visualizations\": [\n",
    "        \"test_performance_comparison.png\",\n",
    "        \"improvement_vs_baseline.png\",\n",
    "        \"pseudo_labels_comparison.png\",\n",
    "        \"training_data_composition.png\",\n",
    "        \"learning_curves.png\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "dashboard_file = results_dir / \"dashboard_summary.json\"\n",
    "with open(dashboard_file, \"w\") as f:\n",
    "    json.dump(dashboard_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Dashboard summary saved to: {dashboard_file}\")\n",
    "display(Markdown(f\"## Experiment Complete! ‚úÖ\\n\\nAll results saved to: `{results_dir}`\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
