{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "113e0fa6",
   "metadata": {},
   "source": [
    "# So SÃ¡nh CÃ¡c MÃ´ HÃ¬nh - Model Comparison\n",
    "\n",
    "## Má»¥c TiÃªu (OPTIMIZED)\n",
    "- So sÃ¡nh **2 thuáº­t toÃ¡n** trong self-training:\n",
    "  1. **HistGradientBoostingClassifier** (baseline hiá»‡n táº¡i)\n",
    "  2. **RandomForestClassifier** (ensemble method)\n",
    "- Xem model nÃ o hÆ°á»Ÿng lá»£i nhiá»u nháº¥t tá»« self-training\n",
    "- PhÃ¢n tÃ­ch probability calibration vÃ  confidence patterns\n",
    "\n",
    "**âš¡ Time Optimization**: Giáº£m tá»« 3 â†’ 2 models (saves ~7-8 minutes)\n",
    "- Bá» XGBoost (tÆ°Æ¡ng tá»± HGBC, cÃ¹ng gradient boosting â†’ Ã­t contrast)\n",
    "- Giá»¯ HGBC vs RandomForest = **Gradient Boosting vs Bagging** â†’ contrast RÃ• RÃ€NG\n",
    "\n",
    "## Giáº£ Thuyáº¿t\n",
    "- **HGBC**: Tá»‘t cho tabular data, probabilities well-calibrated\n",
    "- **RandomForest**: CÃ³ thá»ƒ overconfident â†’ cáº§n Ï„ cao hÆ¡n, hoáº·c performance kÃ©m\n",
    "\n",
    "## Setup\n",
    "- Labeled fraction: 5% (cá»‘ Ä‘á»‹nh)\n",
    "- NgÆ°á»¡ng Ï„ = 0.90 (cá»‘ Ä‘á»‹nh)\n",
    "- Max iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fea612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "SEMI_DATASET_PATH = \"data/processed/dataset_for_semi.parquet\"\n",
    "CUTOFF = \"2017-01-01\"\n",
    "\n",
    "# Fixed parameters\n",
    "LABELED_FRACTION = 0.05  # Use 5% labeled (95% missing)\n",
    "TAU = 0.90\n",
    "MAX_ITER = 10\n",
    "MIN_NEW_PER_ITER = 20\n",
    "VAL_FRAC = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Output directory\n",
    "RESULTS_DIR = \"data/processed/model_comparison_experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c5b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "from src.semi_supervised_library import (\n",
    "    SemiDataConfig, SelfTrainingConfig, AQI_CLASSES,\n",
    "    time_split, build_feature_columns, _normalize_missing, _align_proba_to_labels\n",
    ")\n",
    "\n",
    "# Try importing XGBoost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGBOOST = True\n",
    "except ImportError:\n",
    "    HAS_XGBOOST = False\n",
    "    print(\"âš ï¸ XGBoost not installed. Will skip XGBoost comparison.\")\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "if not (PROJECT_ROOT / \"data\").exists() and (PROJECT_ROOT.parent / \"data\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent.resolve()\n",
    "\n",
    "results_dir = (PROJECT_ROOT / RESULTS_DIR).resolve()\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Results directory: {results_dir}\")\n",
    "print(f\"XGBoost available: {HAS_XGBOOST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70974599",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (already has 5% labeled by default)\n",
    "df = pd.read_parquet((PROJECT_ROOT / SEMI_DATASET_PATH).resolve())\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Labeled fraction:\", df['is_labeled'].mean())\n",
    "\n",
    "train_df, test_df = time_split(df, cutoff=CUTOFF)\n",
    "print(f\"\\nTrain: {len(train_df):,} samples\")\n",
    "print(f\"  - Labeled: {train_df['is_labeled'].sum():,}\")\n",
    "print(f\"  - Unlabeled: {(~train_df['is_labeled']).sum():,}\")\n",
    "print(f\"Test: {len(test_df):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89594c",
   "metadata": {},
   "source": [
    "## Define Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3245bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations (OPTIMIZED: 2 models vá»›i contrast cao nháº¥t)\n",
    "model_configs = {\n",
    "    \"HistGradientBoosting\": {\n",
    "        \"model_class\": HistGradientBoostingClassifier,\n",
    "        \"params\": {\n",
    "            \"max_iter\": 100,\n",
    "            \"max_depth\": 10,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"random_state\": RANDOM_STATE\n",
    "        },\n",
    "        \"color\": \"steelblue\"\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model_class\": RandomForestClassifier,\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 100,\n",
    "            \"max_depth\": 15,\n",
    "            \"min_samples_split\": 10,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "            \"n_jobs\": -1\n",
    "        },\n",
    "        \"color\": \"forestgreen\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# XGBoost bá» qua Ä‘á»ƒ giáº£m thá»i gian (~7-8 phÃºt)\n",
    "# LÃ½ do: XGBoost tÆ°Æ¡ng tá»± HGBC (cÃ¹ng gradient boosting), Ã­t contrast\n",
    "# HGBC vs RandomForest = Gradient Boosting vs Bagging â†’ contrast RÃ• RÃ€NG\n",
    "\n",
    "if HAS_XGBOOST:\n",
    "    print(\"âš ï¸ XGBoost available but SKIPPED for faster execution\")\n",
    "    print(\"   (XGBoost similar to HGBC, less contrast)\\n\")\n",
    "\n",
    "print(\"Models to compare (optimized - 2 architectures):\")\n",
    "for name in model_configs.keys():\n",
    "    print(f\"  âœ“ {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c343c2f",
   "metadata": {},
   "source": [
    "## Custom Self-Training Loop for Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764929f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self_training_with_model(df, model_name, model_class, model_params, tau=0.90, max_iter=10):\n",
    "    \"\"\"Custom self-training implementation to use different models\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING: {model_name} (Ï„={tau})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    data_cfg = SemiDataConfig(cutoff=CUTOFF, random_state=RANDOM_STATE)\n",
    "    \n",
    "    train_df, test_df = time_split(df.copy(), cutoff=CUTOFF)\n",
    "    feat_cols = build_feature_columns(train_df, data_cfg)\n",
    "    \n",
    "    X_all = _normalize_missing(train_df[feat_cols].copy())\n",
    "    y_all = train_df[data_cfg.target_col].astype(\"object\")\n",
    "    \n",
    "    # Split labeled into fit and validation\n",
    "    labeled_idx = train_df.index[pd.notna(y_all)].to_numpy()\n",
    "    unlabeled_idx = train_df.index[pd.isna(y_all)].to_numpy()\n",
    "    \n",
    "    rng = np.random.default_rng(RANDOM_STATE)\n",
    "    rng.shuffle(labeled_idx)\n",
    "    n_val = int(np.floor(VAL_FRAC * labeled_idx.size))\n",
    "    val_idx = labeled_idx[:n_val]\n",
    "    fit_idx = labeled_idx[n_val:]\n",
    "    \n",
    "    print(f\"  Fit set: {len(fit_idx):,}\")\n",
    "    print(f\"  Val set: {len(val_idx):,}\")\n",
    "    print(f\"  Unlabeled: {len(unlabeled_idx):,}\")\n",
    "    \n",
    "    # Build pipeline\n",
    "    cat_cols = [c for c in feat_cols if train_df[c].dtype == \"object\"]\n",
    "    num_cols = [c for c in feat_cols if c not in cat_cols]\n",
    "    \n",
    "    if cat_cols:\n",
    "        encoder = ColumnTransformer([\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols)\n",
    "        ], remainder=\"passthrough\")\n",
    "    else:\n",
    "        encoder = \"passthrough\"\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        (\"encoder\", encoder),\n",
    "        (\"model\", model_class(**model_params))\n",
    "    ])\n",
    "    \n",
    "    # Self-training loop\n",
    "    y_work = y_all.copy()\n",
    "    history = []\n",
    "    \n",
    "    for it in range(1, max_iter + 1):\n",
    "        # Fit\n",
    "        pipe.fit(X_all.loc[fit_idx], y_work.loc[fit_idx])\n",
    "        \n",
    "        # Validate\n",
    "        y_val_pred = pipe.predict(X_all.loc[val_idx])\n",
    "        val_acc = float(accuracy_score(y_all.loc[val_idx], y_val_pred))\n",
    "        val_f1 = float(f1_score(y_all.loc[val_idx], y_val_pred, average=\"macro\"))\n",
    "        \n",
    "        # Pseudo-label\n",
    "        if unlabeled_idx.size > 0:\n",
    "            proba_raw = pipe.predict_proba(X_all.loc[unlabeled_idx])\n",
    "            proba = _align_proba_to_labels(proba_raw, pipe.named_steps[\"model\"].classes_, AQI_CLASSES)\n",
    "            max_prob = proba.max(axis=1)\n",
    "            y_hat = np.array(AQI_CLASSES, dtype=object)[proba.argmax(axis=1)]\n",
    "            \n",
    "            pick_mask = max_prob >= tau\n",
    "            picked = unlabeled_idx[pick_mask]\n",
    "            picked_labels = y_hat[pick_mask]\n",
    "        else:\n",
    "            picked = np.array([], dtype=int)\n",
    "            picked_labels = np.array([], dtype=object)\n",
    "        \n",
    "        n_new = int(picked.size)\n",
    "        history.append({\n",
    "            \"iter\": it,\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"val_f1_macro\": val_f1,\n",
    "            \"unlabeled_pool\": int(unlabeled_idx.size),\n",
    "            \"new_pseudo\": n_new,\n",
    "            \"tau\": tau\n",
    "        })\n",
    "        \n",
    "        print(f\"  Iter {it:2d}: Val F1={val_f1:.4f}, Acc={val_acc:.4f}, New pseudo={n_new:,}, Pool={unlabeled_idx.size:,}\")\n",
    "        \n",
    "        if n_new < MIN_NEW_PER_ITER:\n",
    "            print(f\"  âš ï¸ Stopped early: only {n_new} new pseudo-labels < {MIN_NEW_PER_ITER}\")\n",
    "            break\n",
    "        \n",
    "        # Add pseudo-labels\n",
    "        y_work.loc[picked] = picked_labels\n",
    "        fit_idx = np.unique(np.concatenate([fit_idx, picked]))\n",
    "        \n",
    "        picked_set = set(picked.tolist())\n",
    "        unlabeled_idx = np.array([i for i in unlabeled_idx if i not in picked_set], dtype=int)\n",
    "    \n",
    "    # Final test evaluation\n",
    "    X_test = _normalize_missing(test_df[feat_cols].copy())\n",
    "    y_test = test_df[data_cfg.target_col].astype(\"object\")\n",
    "    mask = pd.notna(y_test)\n",
    "    \n",
    "    y_pred = pipe.predict(X_test.loc[mask])\n",
    "    \n",
    "    test_acc = float(accuracy_score(y_test.loc[mask], y_pred))\n",
    "    test_f1 = float(f1_score(y_test.loc[mask], y_pred, average=\"macro\"))\n",
    "    report = classification_report(y_test.loc[mask], y_pred, output_dict=True)\n",
    "    \n",
    "    print(f\"\\n  âœ… Test Results:\")\n",
    "    print(f\"     Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"     F1-macro: {test_f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"history\": history,\n",
    "        \"test_accuracy\": test_acc,\n",
    "        \"test_f1_macro\": test_f1,\n",
    "        \"per_class_report\": report,\n",
    "        \"total_pseudo_labels\": sum([h[\"new_pseudo\"] for h in history]),\n",
    "        \"iterations_completed\": len(history)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bdf621",
   "metadata": {},
   "source": [
    "## Run Experiments for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4ac0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    result = run_self_training_with_model(\n",
    "        df=df,\n",
    "        model_name=model_name,\n",
    "        model_class=config[\"model_class\"],\n",
    "        model_params=config[\"params\"],\n",
    "        tau=TAU,\n",
    "        max_iter=MAX_ITER\n",
    "    )\n",
    "    \n",
    "    results[model_name] = result\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL MODEL EXPERIMENTS COMPLETED\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d3d35b",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005045a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "output_file = results_dir / \"model_comparison_results.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Saved results to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d437c89",
   "metadata": {},
   "source": [
    "## Create Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cee7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "for model_name, res in results.items():\n",
    "    summary_data.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Test Accuracy\": res[\"test_accuracy\"],\n",
    "        \"Test F1-macro\": res[\"test_f1_macro\"],\n",
    "        \"Pseudo-labels\": res[\"total_pseudo_labels\"],\n",
    "        \"Iterations\": res[\"iterations_completed\"],\n",
    "        \"Val F1 Peak\": max([h[\"val_f1_macro\"] for h in res[\"history\"]])\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values(\"Test F1-macro\", ascending=False)\n",
    "\n",
    "print(\"\\nðŸ“Š SUMMARY TABLE:\")\n",
    "print(\"=\"*100)\n",
    "display(summary_df)\n",
    "\n",
    "# Save summary\n",
    "summary_csv = results_dir / \"model_comparison_summary.csv\"\n",
    "summary_df.to_csv(summary_csv, index=False)\n",
    "print(f\"\\nâœ… Saved summary to: {summary_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dbd6fd",
   "metadata": {},
   "source": [
    "## Visualization 1: Test Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf30cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "models = summary_df[\"Model\"].tolist()\n",
    "accuracies = summary_df[\"Test Accuracy\"].tolist()\n",
    "f1_scores = summary_df[\"Test F1-macro\"].tolist()\n",
    "colors = [model_configs[m][\"color\"] for m in models]\n",
    "\n",
    "# Accuracy\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax1.set_ylabel(\"Test Accuracy\", fontsize=12, fontweight='bold')\n",
    "ax1.set_title(\"Test Accuracy by Model\", fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim([0.50, 0.65])\n",
    "\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.4f}',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# F1-macro\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(models, f1_scores, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax2.set_ylabel(\"Test F1-macro\", fontsize=12, fontweight='bold')\n",
    "ax2.set_title(\"Test F1-macro by Model\", fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_ylim([0.45, 0.60])\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.4f}',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = results_dir / \"test_performance_by_model.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… Saved plot: {plot_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08371c6",
   "metadata": {},
   "source": [
    "## Visualization 2: Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f735c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for model_name, res in results.items():\n",
    "    history = res[\"history\"]\n",
    "    iterations = [h[\"iter\"] for h in history]\n",
    "    val_f1 = [h[\"val_f1_macro\"] for h in history]\n",
    "    color = model_configs[model_name][\"color\"]\n",
    "    \n",
    "    ax.plot(iterations, val_f1, marker='o', linewidth=2.5, \n",
    "            label=model_name, color=color, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"Iteration\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"Validation F1-macro\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Validation Learning Curves by Model\", fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12, loc='best')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xticks(range(1, 11))\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = results_dir / \"learning_curves_by_model.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… Saved plot: {plot_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dba6ae",
   "metadata": {},
   "source": [
    "## Visualization 3: Pseudo-labeling Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de81c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = summary_df[\"Model\"].tolist()\n",
    "pseudo_counts = summary_df[\"Pseudo-labels\"].tolist()\n",
    "colors_list = [model_configs[m][\"color\"] for m in models]\n",
    "\n",
    "bars = ax.bar(models, pseudo_counts, color=colors_list, alpha=0.8, edgecolor='black')\n",
    "ax.set_ylabel(\"Total Pseudo-labels Added\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Pseudo-labeling Activity by Model\", fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height):,}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = results_dir / \"pseudo_labeling_by_model.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… Saved plot: {plot_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d541dd7",
   "metadata": {},
   "source": [
    "## Visualization 4: Per-Class F1 Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad866bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract per-class F1 scores\n",
    "per_class_data = []\n",
    "for model_name, res in results.items():\n",
    "    row = {\"Model\": model_name}\n",
    "    for aqi_class in AQI_CLASSES:\n",
    "        if aqi_class in res[\"per_class_report\"]:\n",
    "            row[aqi_class] = res[\"per_class_report\"][aqi_class][\"f1-score\"]\n",
    "    per_class_data.append(row)\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_data)\n",
    "per_class_df = per_class_df.set_index(\"Model\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.heatmap(per_class_df, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "            vmin=0, vmax=0.8, ax=ax, cbar_kws={'label': 'F1-score'})\n",
    "ax.set_title(\"Per-Class F1-score Comparison\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"AQI Class\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"Model\", fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = results_dir / \"per_class_f1_heatmap.png\"\n",
    "plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… Saved plot: {plot_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df122473",
   "metadata": {},
   "source": [
    "## Analysis & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9163a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ðŸ“Š KEY FINDINGS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "best_idx = summary_df[\"Test F1-macro\"].idxmax()\n",
    "best_model = summary_df.loc[best_idx]\n",
    "\n",
    "print(f\"\\nðŸ† Best Model: {best_model['Model']}\")\n",
    "print(f\"   Test F1-macro: {best_model['Test F1-macro']:.4f}\")\n",
    "print(f\"   Test Accuracy: {best_model['Test Accuracy']:.4f}\")\n",
    "print(f\"   Pseudo-labels: {best_model['Pseudo-labels']:,}\")\n",
    "print(f\"   Val F1 Peak: {best_model['Val F1 Peak']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Ranking by F1-macro:\")\n",
    "for i, row in summary_df.iterrows():\n",
    "    print(f\"   {i+1}. {row['Model']}: {row['Test F1-macro']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Model Behaviors:\")\n",
    "for model_name, res in results.items():\n",
    "    peak_iter = max(res[\"history\"], key=lambda x: x[\"val_f1_macro\"])\n",
    "    print(f\"   {model_name}:\")\n",
    "    print(f\"      - Peak val F1: {peak_iter['val_f1_macro']:.4f} (iter {peak_iter['iter']})\")\n",
    "    print(f\"      - Pseudo-labels/iter: {res['total_pseudo_labels'] / res['iterations_completed']:.0f} avg\")\n",
    "\n",
    "print(f\"\\nâœ… All visualizations saved to: {results_dir}\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e91aa",
   "metadata": {},
   "source": [
    "## Dashboard Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_data = {\n",
    "    \"experiment_type\": \"model_comparison\",\n",
    "    \"parameters\": {\n",
    "        \"tau\": TAU,\n",
    "        \"max_iter\": MAX_ITER,\n",
    "        \"labeled_fraction\": LABELED_FRACTION,\n",
    "        \"models\": list(model_configs.keys())\n",
    "    },\n",
    "    \"summary\": summary_df.to_dict(orient='records'),\n",
    "    \"best_model\": {\n",
    "        \"name\": best_model[\"Model\"],\n",
    "        \"f1_macro\": float(best_model[\"Test F1-macro\"]),\n",
    "        \"accuracy\": float(best_model[\"Test Accuracy\"])\n",
    "    },\n",
    "    \"visualizations\": [\n",
    "        \"test_performance_by_model.png\",\n",
    "        \"learning_curves_by_model.png\",\n",
    "        \"pseudo_labeling_by_model.png\",\n",
    "        \"per_class_f1_heatmap.png\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "dashboard_file = results_dir / \"dashboard_summary.json\"\n",
    "with open(dashboard_file, \"w\") as f:\n",
    "    json.dump(dashboard_data, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Dashboard summary saved to: {dashboard_file}\")\n",
    "display(Markdown(f\"## Experiment Complete! âœ…\\n\\nResults: `{results_dir}`\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
