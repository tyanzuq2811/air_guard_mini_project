# ğŸ“˜ Giáº£i ThÃ­ch Chi Tiáº¿t Self-Training vÃ  NgÆ°á»¡ng Ï„ (TAU)

## ğŸ¯ Má»¥c ÄÃ­ch Document NÃ y

Document nÃ y giáº£i thÃ­ch má»™t cÃ¡ch **Dá»„ HIá»‚U NHáº¤T** vá»:
1. Self-Training lÃ  gÃ¬?
2. NgÆ°á»¡ng Ï„ (tau) lÃ  gÃ¬ vÃ  táº¡i sao quan trá»ng?
3. CÃ¡ch cháº¡y thÃ­ nghiá»‡m vá»›i nhiá»u giÃ¡ trá»‹ Ï„
4. CÃ¡ch phÃ¢n tÃ­ch vÃ  Ä‘Ã¡nh giÃ¡ káº¿t quáº£

---

## ğŸ“š Pháº§n 1: Self-Training LÃ  GÃ¬?

### VÃ­ Dá»¥ Äá»i Thá»±c: Há»c ViÃªn vÃ  GiÃ¡o ViÃªn

HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n lÃ  má»™t giÃ¡o viÃªn dáº¡y há»c sinh phÃ¢n loáº¡i cháº¥t lÆ°á»£ng khÃ´ng khÃ­:

**TÃŒNH HUá»NG:**
- Báº¡n cÃ³ **1000 máº«u khÃ´ng khÃ­** cáº§n phÃ¢n loáº¡i
- NhÆ°ng chá»‰ cÃ³ **50 máº«u Ä‘Ã£ Ä‘Æ°á»£c chuyÃªn gia gáº¯n nhÃ£n** (5%)
- 950 máº«u cÃ²n láº¡i chÆ°a cÃ³ nhÃ£n (95%)

**GIáº¢ I PHÃP THÃ”NG THÆ¯á»œNG (Supervised Learning):**
```
GiÃ¡o viÃªn: Chá»‰ dÃ¹ng 50 máº«u cÃ³ nhÃ£n Ä‘á»ƒ dáº¡y â†’ Model yáº¿u vÃ¬ Ã­t data
```

**GIáº¢I PHÃP SELF-TRAINING:**
```
BÆ°á»›c 1: GiÃ¡o viÃªn dáº¡y há»c sinh vá»›i 50 máº«u cÃ³ nhÃ£n
BÆ°á»›c 2: Há»c sinh dá»± Ä‘oÃ¡n 950 máº«u cÃ²n láº¡i
BÆ°á»›c 3: GiÃ¡o viÃªn chá»n nhá»¯ng dá»± Ä‘oÃ¡n "ráº¥t tá»± tin" (â‰¥90% cháº¯c cháº¯n)
BÆ°á»›c 4: ThÃªm nhá»¯ng dá»± Ä‘oÃ¡n tá»± tin nÃ y vÃ o bÃ i giáº£ng
BÆ°á»›c 5: Dáº¡y láº¡i há»c sinh vá»›i data má»Ÿ rá»™ng
BÆ°á»›c 6: Láº·p láº¡i cho Ä‘áº¿n khi khÃ´ng cÃ²n dá»± Ä‘oÃ¡n tá»± tin
```

**Káº¾T QUáº¢:** Há»c sinh há»c Ä‘Æ°á»£c nhiá»u hÆ¡n tá»« 50 â†’ 200 â†’ 500 máº«u!

---

## ğŸšï¸ Pháº§n 2: NgÆ°á»¡ng Ï„ (TAU) LÃ  GÃ¬?

### Äá»‹nh NghÄ©a ÄÆ¡n Giáº£n

**Ï„ (tau)** lÃ  **"Ä‘á»™ cháº¯c cháº¯n tá»‘i thiá»ƒu"** mÃ  model cáº§n cÃ³ Ä‘á»ƒ dá»± Ä‘oÃ¡n cá»§a nÃ³ Ä‘Æ°á»£c tin tÆ°á»Ÿng.

### VÃ­ Dá»¥ Cá»¥ Thá»ƒ

Giáº£ sá»­ model dá»± Ä‘oÃ¡n 1 máº«u khÃ´ng khÃ­:

```python
# Model dá»± Ä‘oÃ¡n xÃ¡c suáº¥t cho 6 lá»›p AQI:
Predictions = {
    "Good": 0.05,              # 5% cháº¯c lÃ  "Good"
    "Moderate": 0.08,           # 8% cháº¯c lÃ  "Moderate"
    "Unhealthy_for_Sensitive_Groups": 0.12,
    "Unhealthy": 0.92,          # 92% cháº¯c lÃ  "Unhealthy" â† MAX
    "Very_Unhealthy": 0.02,
    "Hazardous": 0.01
}

max_confidence = 0.92  # Äá»™ tin cáº­y cao nháº¥t
predicted_label = "Unhealthy"
```

**CÃ‚U Há»I: CÃ³ nÃªn tin vÃ o dá»± Ä‘oÃ¡n nÃ y khÃ´ng?**

**ÄÃP ÃN phá»¥ thuá»™c vÃ o Ï„:**

| NgÆ°á»¡ng Ï„ | Quyáº¿t Äá»‹nh | LÃ½ Do |
|----------|-----------|-------|
| Ï„ = 0.70 | âœ… **CHáº¤P NHáº¬N** | 0.92 â‰¥ 0.70 â†’ Äá»§ tin cáº­y |
| Ï„ = 0.85 | âœ… **CHáº¤P NHáº¬N** | 0.92 â‰¥ 0.85 â†’ Äá»§ tin cáº­y |
| Ï„ = 0.90 | âœ… **CHáº¤P NHáº¬N** | 0.92 â‰¥ 0.90 â†’ Äá»§ tin cáº­y |
| Ï„ = 0.95 | âŒ **Bá» QUA** | 0.92 < 0.95 â†’ ChÆ°a Ä‘á»§ cháº¯c cháº¯n |

---

### Code Thá»±c Táº¿

```python
# Trong vÃ²ng láº·p Self-Training
for iteration in range(1, max_iter + 1):
    # 1. Train model trÃªn labeled data
    model.fit(X_labeled, y_labeled)
    
    # 2. Dá»± Ä‘oÃ¡n trÃªn unlabeled data
    probabilities = model.predict_proba(X_unlabeled)  # Shape: (n_samples, 6)
    max_confidence = probabilities.max(axis=1)        # Láº¥y xÃ¡c suáº¥t cao nháº¥t
    predicted_labels = model.predict(X_unlabeled)
    
    # 3. CHá»ˆ CHá»ŒN nhá»¯ng dá»± Ä‘oÃ¡n cÃ³ confidence â‰¥ Ï„
    confident_mask = (max_confidence >= TAU)
    
    # 4. ThÃªm pseudo-labels vÃ o training set
    X_labeled = concat([X_labeled, X_unlabeled[confident_mask]])
    y_labeled = concat([y_labeled, predicted_labels[confident_mask]])
    
    # 5. Loáº¡i nhá»¯ng máº«u Ä‘Ã£ chá»n khá»i unlabeled pool
    X_unlabeled = X_unlabeled[~confident_mask]
```

---

## âš–ï¸ Pháº§n 3: TÃ¡c Äá»™ng Cá»§a CÃ¡c GiÃ¡ Trá»‹ Ï„ KhÃ¡c Nhau

### 3.1 Ï„ = 0.70 (THáº¤P)

**Äáº·c Ä‘iá»ƒm:**
- Model dá»… "tin" vÃ o dá»± Ä‘oÃ¡n cá»§a mÃ¬nh
- Cháº¥p nháº­n cáº£ nhá»¯ng dá»± Ä‘oÃ¡n cÃ³ Ä‘á»™ tin cáº­y vá»«a pháº£i (70%)

**Æ¯u Ä‘iá»ƒm:** âœ…
- ThÃªm Ä‘Æ°á»£c **Ráº¤T NHIá»€U** pseudo-labels má»—i vÃ²ng
- Táº­n dá»¥ng tá»‘i Ä‘a unlabeled data
- Há»c nhanh, Ã­t vÃ²ng láº·p

**NhÆ°á»£c Ä‘iá»ƒm:** âŒ
- **Rá»¦I RO CAO**: Nhiá»u nhÃ£n SAI Ä‘Æ°á»£c thÃªm vÃ o
- Model há»c theo lá»—i â†’ **Confirmation Bias**
- Validation accuracy cÃ³ thá»ƒ **GIáº¢M** á»Ÿ cÃ¡c vÃ²ng sau
- Overfitting trÃªn pseudo-labels sai

**Khi nÃ o dÃ¹ng:**
- Dataset sáº¡ch, Ã­t noise
- CÃ¡c lá»›p dá»… phÃ¢n biá»‡t
- Model ban Ä‘áº§u Ä‘Ã£ máº¡nh (baseline accuracy cao)

**VÃ­ dá»¥ káº¿t quáº£:**
```
VÃ²ng 1: +5000 pseudo-labels â†’ Val F1: 0.65
VÃ²ng 2: +3000 pseudo-labels â†’ Val F1: 0.68
VÃ²ng 3: +1500 pseudo-labels â†’ Val F1: 0.66 âš ï¸ (giáº£m!)
VÃ²ng 4: +500 pseudo-labels  â†’ Val F1: 0.63 âš ï¸ (giáº£m tiáº¿p!)
â†’ NÃŠN Dá»ªNG á» VÃ’NG 2
```

---

### 3.2 Ï„ = 0.90 (Tá»I Æ¯U)

**Äáº·c Ä‘iá»ƒm:**
- CÃ¢n báº±ng giá»¯a cháº¥t lÆ°á»£ng vÃ  sá»‘ lÆ°á»£ng
- Chá»‰ cháº¥p nháº­n dá»± Ä‘oÃ¡n ráº¥t tá»± tin (90%)

**Æ¯u Ä‘iá»ƒm:** âœ…
- **CÃ‚N Báº°NG** giá»¯a sá»‘ lÆ°á»£ng vÃ  cháº¥t lÆ°á»£ng pseudo-labels
- Ãt nhiá»…u hÆ¡n Ï„ = 0.70
- Validation accuracy **á»”N Äá»ŠNH** hoáº·c tÄƒng dáº§n
- Káº¿t quáº£ test tá»‘t

**NhÆ°á»£c Ä‘iá»ƒm:** âŒ
- Há»c cháº­m hÆ¡n Ï„ = 0.70
- CÃ³ thá»ƒ bá» qua má»™t sá»‘ máº«u khÃ³ nhÆ°ng Ä‘Ãºng

**Khi nÃ o dÃ¹ng:**
- **KHUYáº¾N NGHá»Š Máº¶C Äá»ŠNH** cho háº§u háº¿t bÃ i toÃ¡n
- Dataset cÃ³ Ä‘á»™ phá»©c táº¡p trung bÃ¬nh
- Khi khÃ´ng cháº¯c cháº¯n nÃªn chá»n Ï„ nÃ o

**VÃ­ dá»¥ káº¿t quáº£:**
```
VÃ²ng 1: +800 pseudo-labels  â†’ Val F1: 0.65
VÃ²ng 2: +650 pseudo-labels  â†’ Val F1: 0.69 âœ“
VÃ²ng 3: +500 pseudo-labels  â†’ Val F1: 0.72 âœ“
VÃ²ng 4: +350 pseudo-labels  â†’ Val F1: 0.73 âœ“
VÃ²ng 5: +150 pseudo-labels  â†’ Val F1: 0.74 âœ“
â†’ TÄ‚NG Äá»€U, Káº¾T QUáº¢ Tá»T
```

---

### 3.3 Ï„ = 0.95 (CAO)

**Äáº·c Ä‘iá»ƒm:**
- Model ráº¥t tháº­n trá»ng
- Chá»‰ cháº¥p nháº­n dá»± Ä‘oÃ¡n Cá»°C Ká»² tá»± tin (95%)

**Æ¯u Ä‘iá»ƒm:** âœ…
- Pseudo-labels **Cá»°C Ká»² CHÃNH XÃC**
- Gáº§n nhÆ° khÃ´ng cÃ³ nhiá»…u
- An toÃ n, khÃ´ng bá»‹ overfitting

**NhÆ°á»£c Ä‘iá»ƒm:** âŒ
- ThÃªm Ä‘Æ°á»£c **Ráº¤T ÃT** pseudo-labels má»—i vÃ²ng
- Nhiá»u máº«u khÃ³ bá»‹ bá» qua
- **KhÃ´ng táº­n dá»¥ng háº¿t** unlabeled data
- CÃ³ thá»ƒ dá»«ng sá»›m do khÃ´ng Ä‘á»§ `min_new_per_iter`
- Cáº£i thiá»‡n cháº­m

**Khi nÃ o dÃ¹ng:**
- Dataset cÃ³ nhiá»u noise, khÃ³
- CÃ¡c lá»›p khÃ³ phÃ¢n biá»‡t
- Khi cáº§n Ä‘áº£m báº£o cháº¥t lÆ°á»£ng tuyá»‡t Ä‘á»‘i

**VÃ­ dá»¥ káº¿t quáº£:**
```
VÃ²ng 1: +200 pseudo-labels â†’ Val F1: 0.65
VÃ²ng 2: +120 pseudo-labels â†’ Val F1: 0.67 âœ“
VÃ²ng 3: +50 pseudo-labels  â†’ Val F1: 0.68 âœ“
VÃ²ng 4: +15 pseudo-labels  â†’ Dá»ªNG (< min_new_per_iter=20)
â†’ AN TOÃ€N NHÆ¯NG Há»ŒC CHáº¬M
```

---

## ğŸ“Š Pháº§n 4: Báº£ng So SÃ¡nh Tá»•ng Há»£p

| TiÃªu ChÃ­ | Ï„ = 0.70 | Ï„ = 0.85 | Ï„ = 0.90 | Ï„ = 0.95 |
|----------|----------|----------|----------|----------|
| **Sá»‘ pseudo-labels/vÃ²ng** | Ráº¥t nhiá»u (1000+) | Nhiá»u (500-800) | Vá»«a pháº£i (300-600) | Ãt (50-200) |
| **Cháº¥t lÆ°á»£ng pseudo-labels** | Tháº¥p-Trung bÃ¬nh | Trung bÃ¬nh-KhÃ¡ | KhÃ¡-Tá»‘t | Ráº¥t tá»‘t |
| **Rá»§i ro nhiá»…u** | âš ï¸âš ï¸âš ï¸ Cao | âš ï¸âš ï¸ Trung bÃ¬nh | âš ï¸ Tháº¥p | âœ… Ráº¥t tháº¥p |
| **Tá»‘c Ä‘á»™ há»c** | Nhanh | KhÃ¡ nhanh | Vá»«a | Cháº­m |
| **Val accuracy xu hÆ°á»›ng** | TÄƒng rá»“i giáº£m â†—ï¸â†˜ï¸ | TÄƒng á»•n Ä‘á»‹nh â†—ï¸ | TÄƒng á»•n Ä‘á»‹nh â†—ï¸ | TÄƒng cháº­m â†—ï¸ |
| **Test performance** | Trung bÃ¬nh | KhÃ¡ | Tá»‘t | Tá»‘t (náº¿u Ä‘á»§ vÃ²ng) |
| **Khuyáº¿n nghá»‹** | Thá»­ nghiá»‡m | Backup tá»‘t | â­ **Tá»I Æ¯U** | Dataset khÃ³ |

---

## ğŸ”¬ Pháº§n 5: ThÃ­ Nghiá»‡m Chi Tiáº¿t

### 5.1 Thiáº¿t Láº­p

File notebook: `notebooks/semi_self_training_experiments.ipynb`

```python
# Thá»­ 5 giÃ¡ trá»‹ Ï„
TAU_VALUES = [0.70, 0.80, 0.85, 0.90, 0.95]

# Cá»‘ Ä‘á»‹nh cÃ¡c tham sá»‘ khÃ¡c
MAX_ITER = 10
MIN_NEW_PER_ITER = 20
VAL_FRAC = 0.20
RANDOM_STATE = 42
```

### 5.2 Cháº¡y ThÃ­ Nghiá»‡m

```bash
# Äáº£m báº£o Ä‘Ã£ cÃ³ baseline
cd d:\DataEngineer\DataMining\air_guard_mini_project

# Cháº¡y notebook thÃ­ nghiá»‡m
jupyter notebook notebooks/semi_self_training_experiments.ipynb

# Hoáº·c dÃ¹ng papermill
papermill notebooks/semi_self_training_experiments.ipynb \
    notebooks/runs/self_training_experiments_run.ipynb
```

### 5.3 Káº¿t Quáº£ Sáº½ ÄÆ°á»£c LÆ°u Táº¡i

```
data/processed/self_training_experiments/
â”œâ”€â”€ metrics_tau_0_70.json           # Metrics cho Ï„=0.70
â”œâ”€â”€ metrics_tau_0_80.json           # Metrics cho Ï„=0.80
â”œâ”€â”€ metrics_tau_0_85.json           # Metrics cho Ï„=0.85
â”œâ”€â”€ metrics_tau_0_90.json           # Metrics cho Ï„=0.90
â”œâ”€â”€ metrics_tau_0_95.json           # Metrics cho Ï„=0.95
â”œâ”€â”€ predictions_tau_0_70.csv        # Predictions
â”œâ”€â”€ predictions_tau_0_80.csv
â”œâ”€â”€ predictions_tau_0_85.csv
â”œâ”€â”€ predictions_tau_0_90.csv
â”œâ”€â”€ predictions_tau_0_95.csv
â”œâ”€â”€ comparison_summary.csv          # Báº£ng tá»•ng há»£p so sÃ¡nh
â”œâ”€â”€ test_performance_comparison.png # Biá»ƒu Ä‘á»“ so sÃ¡nh test
â”œâ”€â”€ pseudo_labels_over_iterations.png # Sá»‘ pseudo-labels theo vÃ²ng
â”œâ”€â”€ validation_f1_over_iterations.png # Val F1 theo vÃ²ng
â”œâ”€â”€ comparison_with_baseline.png    # So sÃ¡nh vá»›i baseline
â””â”€â”€ per_class_comparison.png        # So sÃ¡nh theo tá»«ng lá»›p
```

---

## ğŸ“ˆ Pháº§n 6: PhÃ¢n TÃ­ch Káº¿t Quáº£

### 6.1 CÃ¡c Biá»ƒu Äá»“ Quan Trá»ng

#### **Biá»ƒu Ä‘á»“ 1: Sá»‘ Pseudo-Labels Qua CÃ¡c VÃ²ng**

```
Ã nghÄ©a:
- ÄÆ°á»ng cao â†’ ThÃªm nhiá»u pseudo-labels
- Giáº£m dáº§n qua cÃ¡c vÃ²ng â†’ BÃŒNH THÆ¯á»œNG (háº¿t máº«u dá»…)
- TÄƒng lÃªn â†’ Model há»c tá»‘t hÆ¡n, tá»± tin hÆ¡n

Nháº­n xÃ©t:
- Ï„=0.70: VÃ²ng Ä‘áº§u Ráº¤T CAO (5000+), giáº£m nhanh
- Ï„=0.90: á»”n Ä‘á»‹nh, giáº£m Ä‘á»u (800 â†’ 600 â†’ 400...)
- Ï„=0.95: Tháº¥p tá»« Ä‘áº§u, giáº£m nhanh
```

#### **Biá»ƒu Ä‘á»“ 2: Validation F1-macro Qua CÃ¡c VÃ²ng**

```
Ã nghÄ©a:
- TÄƒng Ä‘á»u â†’ Model há»c tá»‘t âœ…
- Giáº£m á»Ÿ vÃ²ng nÃ o â†’ Model há»c sai, overfitting âš ï¸
- Dao Ä‘á»™ng â†’ KhÃ´ng á»•n Ä‘á»‹nh âš ï¸

Nháº­n xÃ©t:
- Ï„=0.70: TÄƒng Ä‘áº¿n vÃ²ng 2, sau Ä‘Ã³ GIáº¢M â†’ Nguy hiá»ƒm
- Ï„=0.90: TÄƒng Ä‘á»u qua cÃ¡c vÃ²ng â†’ LÃ½ tÆ°á»Ÿng âœ…
- Ï„=0.95: TÄƒng cháº­m nhÆ°ng á»•n Ä‘á»‹nh
```

#### **Biá»ƒu Ä‘á»“ 3: So SÃ¡nh Test Performance**

```
Ã nghÄ©a:
- Cá»™t cao hÆ¡n â†’ Káº¿t quáº£ tá»‘t hÆ¡n
- So vá»›i baseline â†’ ÄÃ¡nh giÃ¡ hiá»‡u quáº£ self-training

Má»¥c tiÃªu:
- Äáº¡t â‰¥ 95% baseline vá»›i chá»‰ 5% labels â†’ ThÃ nh cÃ´ng lá»›n
- Äáº¡t â‰¥ 90% baseline â†’ ThÃ nh cÃ´ng
- < 85% baseline â†’ Cáº§n cáº£i thiá»‡n
```

---

### 6.2 Quyáº¿t Äá»‹nh Dá»«ng á» VÃ²ng NÃ o?

**TiÃªu ChÃ­ Dá»«ng:**

1. **Dá»«ng Tá»± Äá»™ng (trong code):**
   ```python
   if new_pseudo_labels < MIN_NEW_PER_ITER:
       break  # KhÃ´ng Ä‘á»§ pseudo-labels tá»± tin
   ```

2. **Early Stopping (nÃªn thÃªm):**
   ```python
   if val_f1[iter] < val_f1[iter-1] < val_f1[iter-2]:
       break  # Val F1 giáº£m 2 vÃ²ng liÃªn tiáº¿p â†’ Overfitting
   ```

3. **Manual Decision:**
   - Xem biá»ƒu Ä‘á»“ Val F1-macro
   - Náº¿u tháº¥y giáº£m â†’ Dá»«ng á»Ÿ vÃ²ng TRÆ¯á»šC Ä‘Ã³
   - VÃ­ dá»¥: Val F1 giáº£m á»Ÿ vÃ²ng 4 â†’ DÃ¹ng model vÃ²ng 3

---

## ğŸ¯ Pháº§n 7: So SÃ¡nh Vá»›i Baseline

### 7.1 CÃ¢u Há»i Cáº§n Tráº£ Lá»i

**1. Self-training cáº£i thiá»‡n/giáº£m bao nhiÃªu so vá»›i baseline?**

```python
# Baseline (100% labels)
baseline_accuracy = 0.8523
baseline_f1_macro = 0.7845

# Self-training (5% labels, Ï„=0.90)
self_training_accuracy = 0.8401
self_training_f1_macro = 0.7712

# ChÃªnh lá»‡ch
diff_accuracy = 0.8401 - 0.8523 = -0.0122 (-1.43%)
diff_f1_macro = 0.7712 - 0.7845 = -0.0133 (-1.70%)

# ÄÃNH GIÃ: âœ… THÃ€NH CÃ”NG!
# Chá»‰ giáº£m < 2% so vá»›i baseline máº·c dÃ¹ chá»‰ dÃ¹ng 5% labels
```

**2. Nhá»¯ng lá»›p nÃ o Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i?**

| Lá»›p AQI | Baseline F1 | Self-Train F1 | ChÃªnh lá»‡ch | Nháº­n xÃ©t |
|---------|-------------|---------------|------------|----------|
| Good | 0.89 | 0.88 | -0.01 | Giáº£m nháº¹ |
| Moderate | 0.82 | 0.84 | **+0.02** | âœ… **Cáº£i thiá»‡n** |
| Unhealthy_for_Sensitive | 0.75 | 0.77 | **+0.02** | âœ… **Cáº£i thiá»‡n** |
| Unhealthy | 0.71 | 0.69 | -0.02 | Giáº£m nháº¹ |
| Very_Unhealthy | 0.68 | 0.70 | **+0.02** | âœ… **Cáº£i thiá»‡n** |
| Hazardous | 0.65 | 0.64 | -0.01 | Giáº£m nháº¹ |

**NHáº¬N XÃ‰T:**
- CÃ¡c lá»›p **trung bÃ¬nh** (Moderate, Unhealthy_for_Sensitive) Ä‘Æ°á»£c cáº£i thiá»‡n
- CÃ¡c lá»›p **cá»±c trá»‹** (Good, Hazardous) giáº£m nháº¹ (do Ã­t data)

---

## ğŸ“ Pháº§n 8: Checklist YÃªu Cáº§u Äá» BÃ i

### âœ… YÃªu Cáº§u 1: Thiáº¿t Láº­p ThÃ´ng Sá»‘

- [x] Thá»­ nhiá»u giÃ¡ trá»‹ Ï„ (0.70, 0.80, 0.85, 0.90, 0.95)
- [x] Cháº¡y self-training cho má»—i Ï„
- [x] LÆ°u metrics/predictions cho má»—i Ï„
- [x] So sÃ¡nh vÃ  chá»n Ï„ tá»‘i Æ°u

### âœ… YÃªu Cáº§u 2: LÆ°u Káº¿t Quáº£ vÃ  Biá»ƒu Äá»“

- [x] **Báº£ng diá»…n biáº¿n:** History dataframe cho má»—i Ï„
- [x] **Biá»ƒu Ä‘á»“ 1:** Sá»‘ pseudo-labels qua cÃ¡c vÃ²ng
- [x] **Biá»ƒu Ä‘á»“ 2:** Validation F1-macro qua cÃ¡c vÃ²ng
- [x] **Nháº­n xÃ©t:**
  - VÃ²ng Ä‘áº§u thÃªm bao nhiÃªu?
  - Xu hÆ°á»›ng tÄƒng/giáº£m?
  - Validation cÃ³ giáº£m khÃ´ng? Táº¡i sao?
  - NÃªn dá»«ng á»Ÿ vÃ²ng nÃ o?

### âœ… YÃªu Cáº§u 3: Hiá»‡u NÄƒng MÃ´ HÃ¬nh

- [x] **Accuracy** trÃªn test set
- [x] **F1-score macro** trÃªn test set
- [x] So sÃ¡nh vá»›i baseline
- [x] Nháº­n xÃ©t cáº£i thiá»‡n/giáº£m bao nhiÃªu
- [x] Chá»‰ rÃµ lá»›p nÃ o Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i

---

## ğŸš€ Pháº§n 9: BÆ°á»›c Tiáº¿p Theo

### 1. Cháº¡y Notebook ThÃ­ Nghiá»‡m

```bash
cd d:\DataEngineer\DataMining\air_guard_mini_project
jupyter notebook notebooks/semi_self_training_experiments.ipynb
```

### 2. PhÃ¢n TÃ­ch Káº¿t Quáº£

- Xem cÃ¡c biá»ƒu Ä‘á»“ Ä‘Ã£ táº¡o trong `data/processed/self_training_experiments/`
- Äá»c file `comparison_summary.csv`
- Chá»n Ï„ tá»‘i Æ°u

### 3. Viáº¿t BÃ¡o CÃ¡o

Sá»­ dá»¥ng template sau:

```markdown
## 1. Thiáº¿t Láº­p ThÃ­ Nghiá»‡m

- Thá»­ Ï„ = [0.70, 0.80, 0.85, 0.90, 0.95]
- MAX_ITER = 10
- MIN_NEW_PER_ITER = 20
- Labeled data: 5%

## 2. Káº¿t Quáº£

### 2.1 So SÃ¡nh Test Performance

[ChÃ¨n biá»ƒu Ä‘á»“: test_performance_comparison.png]

â†’ Ï„ = 0.90 Ä‘áº¡t káº¿t quáº£ tá»‘t nháº¥t: F1-macro = 0.7712

### 2.2 Diá»…n Biáº¿n Qua CÃ¡c VÃ²ng

[ChÃ¨n biá»ƒu Ä‘á»“: pseudo_labels_over_iterations.png]
[ChÃ¨n biá»ƒu Ä‘á»“: validation_f1_over_iterations.png]

**Nháº­n xÃ©t:**
- VÃ²ng Ä‘áº§u: Ï„=0.70 thÃªm 5000+ labels (quÃ¡ nhiá»u), Ï„=0.90 thÃªm 800 (há»£p lÃ½)
- Xu hÆ°á»›ng: Giáº£m dáº§n (bÃ¬nh thÆ°á»ng - háº¿t máº«u dá»…)
- Validation: Ï„=0.90 tÄƒng Ä‘á»u, Ï„=0.70 giáº£m tá»« vÃ²ng 3 (overfitting)
- Quyáº¿t Ä‘á»‹nh dá»«ng: Ï„=0.90 tá»± dá»«ng sau 5 vÃ²ng (khÃ´ng Ä‘á»§ confident samples)

### 2.3 So SÃ¡nh Vá»›i Baseline

[ChÃ¨n biá»ƒu Ä‘á»“: comparison_with_baseline.png]

| Metric | Baseline (100%) | Self-Train (5%) | ChÃªnh lá»‡ch |
|--------|-----------------|-----------------|------------|
| Accuracy | 0.8523 | 0.8401 | -1.43% |
| F1-macro | 0.7845 | 0.7712 | -1.70% |

â†’ Self-training Ä‘áº¡t 98.3% hiá»‡u suáº¥t baseline vá»›i chá»‰ 5% labels!

### 2.4 Per-Class Analysis

[ChÃ¨n biá»ƒu Ä‘á»“: per_class_comparison.png]

**Lá»›p Ä‘Æ°á»£c cáº£i thiá»‡n:**
- Moderate: +0.02
- Unhealthy_for_Sensitive_Groups: +0.02
- Very_Unhealthy: +0.02

**Lá»›p bá»‹ giáº£m:**
- Good: -0.01 (Ã­t data trong labeled set)
- Hazardous: -0.01 (lá»›p hiáº¿m)

## 3. Káº¿t Luáº­n

- âœ… NgÆ°á»¡ng tá»‘i Æ°u: **Ï„ = 0.90**
- âœ… Self-training thÃ nh cÃ´ng vá»›i chá»‰ 5% labels
- âœ… Chá»‰ giáº£m < 2% so vá»›i baseline
- âš ï¸ Cáº§n cáº£i thiá»‡n cho cÃ¡c lá»›p hiáº¿m (Hazardous)
```

### 4. Tiáº¿n HÃ nh Co-Training

- Sá»­ dá»¥ng Ï„ = 0.90 (Ä‘Ã£ tá»‘i Æ°u tá»« self-training)
- Cháº¡y notebook `semi_co_training.ipynb`
- So sÃ¡nh Self-Training vs Co-Training

---

## ğŸ’¡ Pháº§n 10: Tips vÃ  Best Practices

### 1. Debugging

```python
# In ra confidence distribution
print("Confidence distribution:")
print(f"  â‰¥ 0.95: {(max_confidence >= 0.95).sum()}")
print(f"  â‰¥ 0.90: {(max_confidence >= 0.90).sum()}")
print(f"  â‰¥ 0.85: {(max_confidence >= 0.85).sum()}")
print(f"  â‰¥ 0.70: {(max_confidence >= 0.70).sum()}")
```

### 2. Monitoring

```python
# Watch for overfitting
if val_f1_current < val_f1_previous:
    print(f"âš ï¸ Warning: Val F1 decreased at iteration {iter}")
    print(f"   Previous: {val_f1_previous:.4f}")
    print(f"   Current:  {val_f1_current:.4f}")
```

### 3. Visualization Tips

- DÃ¹ng `plt.savefig(..., dpi=300)` cho biá»ƒu Ä‘á»“ cháº¥t lÆ°á»£ng cao
- ThÃªm grid: `ax.grid(True, alpha=0.3)`
- Annotate giÃ¡ trá»‹: `ax.text(x, y, f"{value:.4f}")`

---

## ğŸ“ LiÃªn Há»‡ vÃ  Há»— Trá»£

Náº¿u cÃ³ tháº¯c máº¯c, xem láº¡i:
1. Notebook: `notebooks/semi_self_training_experiments.ipynb`
2. Source code: `src/semi_supervised_library.py`
3. Document nÃ y: `SELF_TRAINING_EXPLAINED.md`

---

**ğŸ‰ ChÃºc báº¡n thÃ nh cÃ´ng vá»›i Self-Training!**
