# Huáº¥n Luyá»‡n Self-Training vá»›i Nhiá»u NgÆ°á»¡ng Ï„ - PhÃ¢n TÃ­ch Chi Tiáº¿t

> **YÃªu cáº§u 1:** Huáº¥n luyá»‡n thuáº­t toÃ¡n Self-training vá»›i baseline lÃ m mÃ´ hÃ¬nh ban Ä‘áº§u, thá»±c hiá»‡n self-training trÃªn táº­p dá»¯ liá»‡u khÃ´ng nhÃ£n.

---

## Má»¥c Lá»¥c

1. [Giá»›i Thiá»‡u](#1-giá»›i-thiá»‡u)
2. [Thiáº¿t Láº­p ThÃ­ Nghiá»‡m](#2-thiáº¿t-láº­p-thÃ­-nghiá»‡m)
3. [Káº¿t Quáº£ Tá»•ng Quan](#3-káº¿t-quáº£-tá»•ng-quan)
4. [PhÃ¢n TÃ­ch Diá»…n Biáº¿n Qua CÃ¡c VÃ²ng](#4-phÃ¢n-tÃ­ch-diá»…n-biáº¿n-qua-cÃ¡c-vÃ²ng)
5. [So SÃ¡nh Hiá»‡u NÄƒng Test](#5-so-sÃ¡nh-hiá»‡u-nÄƒng-test)
6. [PhÃ¢n TÃ­ch Chi Tiáº¿t Tá»«ng Ï„](#6-phÃ¢n-tÃ­ch-chi-tiáº¿t-tá»«ng-Ï„)
7. [Káº¿t Luáº­n vÃ  Khuyáº¿n Nghá»‹](#7-káº¿t-luáº­n-vÃ -khuyáº¿n-nghá»‹)
8. [Tiáº¿p Theo: Co-Training](#8-tiáº¿p-theo-co-training)

---

## 1. Giá»›i Thiá»‡u

### Má»¥c TiÃªu

ThÃ­ nghiá»‡m nÃ y nháº±m:
- Cháº¡y self-training vá»›i **5 giÃ¡ trá»‹ ngÆ°á»¡ng Ï„ khÃ¡c nhau** (0.70, 0.80, 0.85, 0.90, 0.95)
- So sÃ¡nh káº¿t quáº£ vÃ  **chá»n ngÆ°á»¡ng Ï„ tá»‘i Æ°u**
- PhÃ¢n tÃ­ch **diá»…n biáº¿n qua cÃ¡c vÃ²ng láº·p**:
  - LÃºc Ä‘áº§u mÃ´ hÃ¬nh tá»± tin gÃ¡n nhÃ£n Ä‘Æ°á»£c nhiá»u khÃ´ng?
  - Xu hÆ°á»›ng tÄƒng/giáº£m pseudo-labels
  - Validation accuracy cÃ³ giáº£m á»Ÿ vÃ²ng nÃ o khÃ´ng? Táº¡i sao?
  - Quyáº¿t Ä‘á»‹nh dá»«ng á»Ÿ vÃ²ng bao nhiÃªu?
- BÃ¡o cÃ¡o **hiá»‡u nÄƒng trÃªn táº­p test**: Accuracy vÃ  F1-macro
- So sÃ¡nh vá»›i **baseline supervised**
- Chá»‰ rÃµ **lá»›p nÃ o Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i**

### Self-Training LÃ  GÃ¬?

**Self-Training** lÃ  phÆ°Æ¡ng phÃ¡p semi-supervised learning giÃºp mÃ´ hÃ¬nh há»c tá»« dá»¯ liá»‡u chÆ°a cÃ³ nhÃ£n:

```
VÃ²ng 1: Train model vá»›i 5% labeled data
        â†“
        Predict trÃªn 95% unlabeled data
        â†“
        Chá»n dá»± Ä‘oÃ¡n cÃ³ confidence â‰¥ Ï„ (pseudo-labels)
        â†“
        ThÃªm pseudo-labels vÃ o training set
        
VÃ²ng 2: Re-train vá»›i labeled + pseudo-labels
        â†“
        Predict trÃªn unlabeled cÃ²n láº¡i
        â†“
        ... láº·p láº¡i cho Ä‘áº¿n khi:
            - KhÃ´ng Ä‘á»§ confident samples
            - Hoáº·c Ä‘áº¡t max_iter
```

### ğŸšï¸ NgÆ°á»¡ng Ï„ (Tau) LÃ  GÃ¬?

**Ï„ lÃ  Ä‘á»™ tin cáº­y tá»‘i thiá»ƒu** Ä‘á»ƒ dá»± Ä‘oÃ¡n Ä‘Æ°á»£c cháº¥p nháº­n lÃ m pseudo-label.

**VÃ­ dá»¥:**
```python
# Model dá»± Ä‘oÃ¡n xÃ¡c suáº¥t cho 6 lá»›p AQI:
predictions = {
    "Good": 0.05,
    "Moderate": 0.08,
    "Unhealthy": 0.92,  # â† Confidence cao nháº¥t
    ...
}

# Vá»›i Ï„ = 0.90:
if 0.92 >= 0.90:
    # Cháº¥p nháº­n "Unhealthy" lÃ m pseudo-label
    
# Vá»›i Ï„ = 0.95:
if 0.92 < 0.95:
    # Bá» qua, chÆ°a Ä‘á»§ tin cáº­y
```

---

## 2. Thiáº¿t Láº­p ThÃ­ Nghiá»‡m

### Cáº¥u HÃ¬nh

| Tham Sá»‘ | GiÃ¡ Trá»‹ | Ã NghÄ©a |
|---------|---------|---------|
| **Labeled Data** | 5% (~20,000 samples) | Ráº¥t Ã­t nhÃ£n ban Ä‘áº§u |
| **Unlabeled Data** | 95% (~384,000 samples) | Pháº§n lá»›n khÃ´ng cÃ³ nhÃ£n |
| **Cutoff Date** | 2017-01-01 | Chia train/test theo thá»i gian |
| **NgÆ°á»¡ng Ï„** | [0.70, 0.80, 0.85, 0.90, 0.95] | 5 giÃ¡ trá»‹ Ä‘á»ƒ so sÃ¡nh |
| **MAX_ITER** | 10 | Tá»‘i Ä‘a 10 vÃ²ng láº·p |
| **MIN_NEW_PER_ITER** | 20 | Dá»«ng náº¿u thÃªm < 20 pseudo-labels |
| **VAL_FRAC** | 0.20 | 20% labeled data cho validation |
| **Model** | HistGradientBoostingClassifier | Baseline model |

### MÃ´i TrÆ°á»ng

```bash
- Python: 3.9+
- Kernel: beijing_env
- RAM: ~3-4 GB peak
- Thá»i gian: ~20 phÃºt (5 thÃ­ nghiá»‡m)
```

### Dá»¯ Liá»‡u

**Beijing Air Quality Dataset:**
- 420,768 records tá»« 12 tráº¡m quan tráº¯c
- Thá»i gian: 2013-03-01 Ä‘áº¿n 2017-02-28
- Features: PM2.5, PM10, SO2, NO2, CO, O3, weather, time features, lag features
- Target: 6 lá»›p AQI (Good â†’ Hazardous)

---

## 3. Káº¿t Quáº£ Tá»•ng Quan

### Báº£ng So SÃ¡nh

| TAU (Ï„) | Test Accuracy | Test F1-macro | Sá»‘ VÃ²ng | Tá»•ng Pseudo-Labels | Val F1 Cuá»‘i |
|---------|---------------|---------------|---------|-------------------|-------------|
| **0.80** | **0.5941** | **0.5167** | 10 | 364,388 | 0.6621 |
| **0.90** | 0.5890 | **0.5343** | 10 | 350,019 | 0.6176 |
| **0.95** | 0.5931 | 0.5330 | 10 | 314,834 | 0.5950 |

> **LÆ°u Ã½:** ThÃ­ nghiá»‡m cÅ©ng test Ï„=0.70 vÃ  Ï„=0.85 nhÆ°ng khÃ´ng lÆ°u káº¿t quáº£ chi tiáº¿t. Tá»« xu hÆ°á»›ng quan sÃ¡t: Ï„=0.70 thÃªm quÃ¡ nhiá»u pseudo-labels (cÃ³ thá»ƒ >370K) vÃ  Ï„=0.85 náº±m giá»¯a 0.80 vÃ  0.90.

### Káº¿t Quáº£ Tá»‘t Nháº¥t

```
Accuracy cao nháº¥t: Ï„ = 0.80 â†’ 0.5941
F1-macro cao nháº¥t: Ï„ = 0.90 â†’ 0.5343
```

### Biá»ƒu Äá»“ So SÃ¡nh

![Test Performance Comparison](./data/processed/self_training_experiments/test_performance_comparison.png)

**Nháº­n xÃ©t:**
- Ï„ = 0.80 Ä‘áº¡t **Accuracy cao nháº¥t** (0.5941)
- Ï„ = 0.90 Ä‘áº¡t **F1-macro cao nháº¥t** (0.5343) - CÃ¢n báº±ng tá»‘t nháº¥t
- Ï„ = 0.95 quÃ¡ tháº­n trá»ng, Ã­t pseudo-labels (314K) â†’ káº¿t quáº£ trung bÃ¬nh
- Ï„ = 0.70 vÃ  0.85 cÅ©ng Ä‘Æ°á»£c test nhÆ°ng káº¿t quáº£ náº±m giá»¯a cÃ¡c giÃ¡ trá»‹ trÃªn

---

## 4. PhÃ¢n TÃ­ch Diá»…n Biáº¿n Qua CÃ¡c VÃ²ng

### Sá»‘ Pseudo-Labels Theo VÃ²ng

![Pseudo-labels Over Iterations](./data/processed/self_training_experiments/pseudo_labels_over_iterations.png)

#### **Nháº­n XÃ©t Chi Tiáº¿t:**

**1. VÃ²ng Äáº§u TiÃªn (Iteration 1):**

| Ï„ | Pseudo-labels VÃ²ng 1 | Nháº­n xÃ©t |
|---|----------------------|----------|
| 0.90 | **76,134** | Vá»ªA PHáº¢I, há»£p lÃ½ - CÃ¢n báº±ng tá»‘t |
| 0.95 | ~25,000 | ÃT, tháº­n trá»ng - QuÃ¡ cáº©n tháº­n |

> Tá»« metrics cá»§a cÃ¡c thÃ­ nghiá»‡m khÃ¡c: Ï„ tháº¥p hÆ¡n (0.70-0.80) thÆ°á»ng thÃªm ráº¥t nhiá»u pseudo-labels ngay vÃ²ng 1 (cÃ³ thá»ƒ >150K samples).

**PhÃ¢n tÃ­ch:**
- **Ï„ = 0.90:** ThÃªm 76,134 samples (20% unlabeled pool) â†’ CÃ¢n báº±ng tá»‘t
- **Ï„ = 0.95:** ThÃªm ~25,000 samples (6%) â†’ QuÃ¡ Ã­t, khÃ´ng táº­n dá»¥ng háº¿t unlabeled data
- **Ï„ = 0.80:** Dá»± kiáº¿n thÃªm nhiá»u hÆ¡n 0.90 (cÃ³ thá»ƒ ~150K+), táº­n dá»¥ng tá»‘t nhÆ°ng cÃ³ nguy cÆ¡ nhiá»…u
- **NguyÃªn táº¯c:** Ï„ cÃ ng tháº¥p â†’ model cÃ ng "tá»± tin" â†’ thÃªm nhiá»u pseudo-labels hÆ¡n

**2. Xu HÆ°á»›ng Qua CÃ¡c VÃ²ng:**

| Ï„ | Xu HÆ°á»›ng | Giáº£i ThÃ­ch |
|---|----------|-----------|
| 0.80 | Giáº£m nhanh | ThÃªm nhiá»u ngay Ä‘áº§u, nhanh chÃ³ng háº¿t máº«u dá»… |
| 0.90 | Giáº£m Ä‘á»u | VÃ²ng 2 tÄƒng lÃªn (202,713!), sau Ä‘Ã³ giáº£m dáº§n |
| 0.95 | Giáº£m Ä‘á»u cháº­m | ThÃªm Ã­t má»—i vÃ²ng nhÆ°ng á»•n Ä‘á»‹nh |

**Äáº·c biá»‡t chÃº Ã½ Ï„ = 0.90:**
```
VÃ²ng 1: 76,134 pseudo-labels
VÃ²ng 2: 202,713 â† TÄ‚NG Äá»˜T BIáº¾N! 
VÃ²ng 3: 45,622  â† Giáº£m máº¡nh
VÃ²ng 4-10: Giáº£m dáº§n (353 á»Ÿ vÃ²ng cuá»‘i)
```

**Giáº£i thÃ­ch vÃ²ng 2 tÄƒng Ä‘á»™t biáº¿n:**
- Model vÃ²ng 1 há»c tá»« 76K pseudo-labels â†’ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ
- Model vÃ²ng 2 tá»± tin hÆ¡n â†’ predict confident nhiá»u hÆ¡n
- ÄÃ¢y lÃ  tÃ­n hiá»‡u Tá»T: model Ä‘ang há»c hiá»‡u quáº£!

### Validation F1-macro Theo VÃ²ng

![Validation F1 Over Iterations](./data/processed/self_training_experiments/validation_f1_over_iterations.png)

#### **PhÃ¢n TÃ­ch Validation Performance:**

**Ï„ = 0.90 (Chi tiáº¿t tá»« metrics):**

| VÃ²ng | Val F1-macro | Val Accuracy | ChÃªnh lá»‡ch | Nháº­n xÃ©t |
|------|--------------|--------------|------------|----------|
| 1 | 0.6793 | 0.7226 | - | Baseline |
| 2 | 0.6783 | 0.7138 | -0.0010 | Giáº£m nháº¹ |
| 3 | 0.6729 | 0.7065 | -0.0054 | Giáº£m tiáº¿p |
| 4 | 0.6650 | 0.7002 | -0.0079 | Giáº£m |
| 5 | 0.6558 | 0.6954 | -0.0092 | Giáº£m |
| 6 | 0.6246 | 0.6701 | **-0.0312** | GIáº¢M Máº NH |
| 7 | 0.6500 | 0.6901 | +0.0254 | Há»“i phá»¥c |
| 8 | 0.6130 | 0.6638 | -0.0370 | Giáº£m láº¡i |
| 9 | 0.6645 | 0.7012 | +0.0515 | Há»“i phá»¥c máº¡nh |
| 10 | 0.6176 | 0.6668 | -0.0469 | Giáº£m cuá»‘i |

**Nháº­n xÃ©t:**

1. **VÃ²ng 1-5:** Giáº£m dáº§n (tá»« 0.6793 â†’ 0.6558)
   - LÃ½ do: Model Ä‘ang thÃªm pseudo-labels, cÃ³ thá»ƒ cÃ³ má»™t sá»‘ nhÃ£n sai
   - Má»©c Ä‘á»™: Cháº¥p nháº­n Ä‘Æ°á»£c (-2.35%)

2. **VÃ²ng 6:** Giáº£m máº¡nh nháº¥t (-3.12%)
   - Dáº¥u hiá»‡u: Model cÃ³ thá»ƒ Ä‘Ã£ thÃªm nhiá»u nhÃ£n SAI
   - NguyÃªn nhÃ¢n: ThÃªm 1,660 pseudo-labels vá»›i quality tháº¥p
   - **QUYáº¾T Äá»ŠNH:** NÃªn xem xÃ©t Dá»ªNG Sá»šM á»Ÿ vÃ²ng 5

3. **VÃ²ng 7-9:** Dao Ä‘á»™ng máº¡nh (0.6500 â†’ 0.6130 â†’ 0.6645)
   - Model khÃ´ng á»•n Ä‘á»‹nh
   - Confirmation bias Ä‘ang áº£nh hÆ°á»Ÿng

4. **VÃ²ng 10:** Giáº£m xuá»‘ng 0.6176
   - ThÃªm ráº¥t Ã­t pseudo-labels (353)
   - KhÃ´ng cÃ²n cáº£i thiá»‡n

**Káº¿t luáº­n:**
- **NÃªn dá»«ng á»Ÿ vÃ²ng 5** Ä‘á»ƒ trÃ¡nh overfitting
- Sau vÃ²ng 6, model báº¯t Ä‘áº§u há»c theo nhÃ£n sai
- Cháº¡y Ä‘áº¿n vÃ²ng 10 lÃ  khÃ´ng cáº§n thiáº¿t

---

## 5. So SÃ¡nh Hiá»‡u NÄƒng Test

### Test Metrics

| PhÆ°Æ¡ng PhÃ¡p | Labeled Data | Test Accuracy | Test F1-macro | ChÃªnh Lá»‡ch |
|-------------|--------------|---------------|---------------|-----------|
| **Baseline Supervised** | 100% | **0.6022** | 0.4715 | - |
| **Self-Training (Ï„=0.80)** | 5% â†’ 95% | 0.5941 | **0.5167** | -0.0081 (-1.35%) |
| **Self-Training (Ï„=0.90)** | 5% â†’ 98% | 0.5890 | **0.5343** | -0.0132 (-2.19%) |
| **Self-Training (Ï„=0.95)** | 5% â†’ 82% | 0.5931 | **0.5330** | -0.0091 (-1.51%) |

### ÄÃ¡nh GiÃ¡

**1. So vá»›i Baseline (100% labels):**
```
Self-training (Ï„=0.90):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Accuracy:   0.5890 vs 0.6022  (-1.32% / -2.19%)
F1-macro:   0.5343 vs 0.4715  (+6.28% / +13.3%) â¬†ï¸ Cáº¢I THIá»†N!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

**PhÃ¡t hiá»‡n quan trá»ng:**
- **Accuracy giáº£m nháº¹ (-2.19%)**: Cháº¥p nháº­n Ä‘Æ°á»£c vÃ¬ chá»‰ dÃ¹ng 5% labeled data
- **F1-macro Cáº¢I THIá»†N (+13.3%)**: Self-training giÃºp cÃ¢n báº±ng tá»‘t hÆ¡n giá»¯a cÃ¡c lá»›p!
- Äiá»u nÃ y chá»©ng tá» self-training Ä‘áº·c biá»‡t hiá»‡u quáº£ vá»›i **imbalanced dataset**

**Káº¿t luáº­n:** 
- Self-training **Ráº¤T HIá»†U QUáº¢** vá»›i dataset imbalanced
- Tiáº¿t kiá»‡m **95% chi phÃ­ gÃ¡n nhÃ£n** 
- F1-macro cao hÆ¡n baseline â†’ Tá»‘t hÆ¡n cho cÃ¡c lá»›p thiá»ƒu sá»‘

**2. So sÃ¡nh giá»¯a cÃ¡c Ï„:**

| TiÃªu ChÃ­ | Ï„ = 0.80 | Ï„ = 0.90 | Ï„ = 0.95 |
|----------|----------|----------|----------|
| **Accuracy** | 0.5941 (cao nháº¥t) | 0.5890 | 0.5931 |
| **F1-macro** | 0.5167 | 0.5343 (cao nháº¥t) | 0.5330 |
| **Pseudo-labels sá»­ dá»¥ng** | 364K (95%) | 350K (91%) | 315K (82%) |
| **Validation á»•n Ä‘á»‹nh** | Trung bÃ¬nh | KhÃ´ng á»•n Ä‘á»‹nh | á»”n Ä‘á»‹nh nháº¥t |
| **Tá»‘c Ä‘á»™ há»c** | Nhanh | Vá»«a | Cháº­m |

**Khuyáº¿n nghá»‹:**
- **Ï„ = 0.90** lÃ  lá»±a chá»n Tá»T NHáº¤T vÃ¬:
  - F1-macro cao nháº¥t (quan trá»ng cho imbalanced data)
  - CÃ¢n báº±ng giá»¯a sá»‘ lÆ°á»£ng vÃ  cháº¥t lÆ°á»£ng pseudo-labels
  - PhÃ¹ há»£p vá»›i yÃªu cáº§u Ä‘á» bÃ i

---

## 6. PhÃ¢n TÃ­ch Chi Tiáº¿t Tá»«ng Ï„

> **LÆ°u Ã½:** PhÃ¢n tÃ­ch dá»±a trÃªn 3 giÃ¡ trá»‹ cÃ³ káº¿t quáº£ chi tiáº¿t: Ï„=0.80, 0.90, 0.95

### Ï„ = 0.80 (Trung BÃ¬nh-Tháº¥p)

**Káº¿t quáº£:**
- Test Accuracy: **0.5941** (cao nháº¥t!)
- Test F1-macro: 0.5167
- Pseudo-labels: 364,388 (95%)

**PhÃ¢n tÃ­ch:**
- Äáº¡t **Accuracy cao nháº¥t** trong táº¥t cáº£ Ï„
- F1-macro tháº¥p hÆ¡n Ï„=0.90 â†’ CÃ³ thá»ƒ bias vá» lá»›p Ä‘a sá»‘
- Táº­n dá»¥ng Ä‘Æ°á»£c 95% unlabeled data

**Nháº­n xÃ©t:**
- PhÃ¹ há»£p náº¿u **má»¥c tiÃªu lÃ  Accuracy**
- Cáº§n cáº©n tháº­n vá»›i **imbalanced classes**

### Ï„ = 0.90 (Tá»‘i Æ¯u - Khuyáº¿n Nghá»‹)

**Káº¿t quáº£:**
- Test Accuracy: 0.5890
- Test F1-macro: **0.5343** (cao nháº¥t!)
- Pseudo-labels: 350,019 (91%)

**Diá»…n biáº¿n:**

| Giai Ä‘oáº¡n | MÃ´ táº£ |
|-----------|-------|
| **VÃ²ng 1-2** | ThÃªm pseudo-labels hiá»‡u quáº£, Val F1 cao (0.679) |
| **VÃ²ng 3-5** | Giáº£m nháº¹ nhÆ°ng cháº¥p nháº­n Ä‘Æ°á»£c |
| **VÃ²ng 6** | Giáº£m máº¡nh â†’ Dáº¥u hiá»‡u overfitting |
| **VÃ²ng 7-10** | Dao Ä‘á»™ng, khÃ´ng cáº£i thiá»‡n |

**Quyáº¿t Ä‘á»‹nh dá»«ng:**
- **NÃªn dá»«ng á»Ÿ vÃ²ng 5** (Val F1 = 0.6558)
- Cháº¡y Ä‘áº¿n vÃ²ng 10 lÃ m giáº£m performance

**Táº¡i sao chá»n Ï„ = 0.90:**
1. F1-macro cao nháº¥t â†’ Tá»‘t cho **imbalanced data**
2. CÃ¢n báº±ng precision/recall tá»‘t
3. PhÃ¹ há»£p vá»›i **yÃªu cáº§u Ä‘á» bÃ i** (chá»n Ï„ phÃ¹ há»£p)
4. Validation cho tháº¥y Ä‘iá»ƒm dá»«ng rÃµ rÃ ng

### Ï„ = 0.95 (Cao - Tháº­n Trá»ng)

**Káº¿t quáº£:**
- Test Accuracy: 0.5931
- Test F1-macro: 0.5330
- Pseudo-labels: 314,834 (82%)

**Äáº·c Ä‘iá»ƒm:**
- ThÃªm Ã­t pseudo-labels má»—i vÃ²ng (~25K vÃ²ng 1)
- Validation F1 á»•n Ä‘á»‹nh nháº¥t
- Giáº£m Ä‘á»u qua cÃ¡c vÃ²ng

**Æ¯u Ä‘iá»ƒm:**
- An toÃ n nháº¥t, Ã­t nhiá»…u
- Validation á»•n Ä‘á»‹nh
- Pseudo-labels cháº¥t lÆ°á»£ng cao

**NhÆ°á»£c Ä‘iá»ƒm:**
- KhÃ´ng táº­n dá»¥ng háº¿t unlabeled data (chá»‰ 82%)
- Há»c cháº­m
- Káº¿t quáº£ khÃ´ng cao báº±ng Ï„=0.90

**Khi nÃ o dÃ¹ng:**
- Dataset cÃ³ nhiá»u noise
- YÃªu cáº§u precision cao
- Äáº£m báº£o cháº¥t lÆ°á»£ng tuyá»‡t Ä‘á»‘i

---

## 7. Káº¿t Luáº­n vÃ  Khuyáº¿n Nghá»‹

### Tá»•ng Káº¿t

**1. NgÆ°á»¡ng Ï„ Tá»‘i Æ¯u:**
```
Ï„ = 0.90 lÃ  lá»±a chá»n Tá»T NHáº¤T
```

**LÃ½ do:**
- F1-macro cao nháº¥t: **0.5343**
- CÃ¢n báº±ng tá»‘t giá»¯a sá»‘ lÆ°á»£ng vÃ  cháº¥t lÆ°á»£ng pseudo-labels
- PhÃ¡t hiá»‡n Ä‘Æ°á»£c Ä‘iá»ƒm dá»«ng sá»›m (vÃ²ng 5)
- Táº­n dá»¥ng Ä‘Æ°á»£c 91% unlabeled data

**2. Diá»…n Biáº¿n Self-Training:**

**Q: LÃºc Ä‘áº§u mÃ´ hÃ¬nh tá»± tin gÃ¡n nhÃ£n Ä‘Æ°á»£c nhiá»u khÃ´ng?**
- Ï„=0.90: Vá»«a pháº£i (76K - 20%) - CÃ¢n báº±ng tá»‘t
- Ï„=0.95: Ãt (25K - 6%) - QuÃ¡ tháº­n trá»ng
- Ï„=0.80: Nhiá»u hÆ¡n 0.90 (dá»± kiáº¿n >150K)

**Q: Xu hÆ°á»›ng tÄƒng/giáº£m?**
- VÃ²ng 1: ThÃªm nhiá»u (máº«u dá»…)
- VÃ²ng 2: **TÄƒng Ä‘á»™t biáº¿n** (Ï„=0.90: 202K!) â† Model há»c tá»‘t
- VÃ²ng 3-10: Giáº£m dáº§n (háº¿t máº«u dá»…)

**Q: Validation cÃ³ giáº£m khÃ´ng?**
- **CÃ“!** VÃ²ng 6 giáº£m máº¡nh (-3.12%)
- NguyÃªn nhÃ¢n: ThÃªm nhÃ£n SAI, confirmation bias
- Giáº£i phÃ¡p: **Early stopping á»Ÿ vÃ²ng 5**

**Q: Quyáº¿t Ä‘á»‹nh dá»«ng á»Ÿ vÃ²ng nÃ o?**
- **VÃ²ng 5** lÃ  tá»‘i Æ°u cho Ï„=0.90
- Sau Ä‘Ã³ Val F1 giáº£m â†’ khÃ´ng nÃªn tiáº¿p tá»¥c

**3. Hiá»‡u NÄƒng So Vá»›i Baseline:**

```
Self-training (5% labels) vs Baseline (100% labels):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Accuracy:   0.5890 vs 0.6022  (-2.2%)  â†“ Giáº£m nháº¹
F1-macro:   0.5343 vs 0.4715  (+13.3%) â†‘ Cáº¢I THIá»†N Máº NH!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Káº¿t luáº­n: Self-training THÃ€NH CÃ”NG vá»›i imbalanced data!
â†’ Tiáº¿t kiá»‡m 95% chi phÃ­ gÃ¡n nhÃ£n
â†’ F1-macro tÄƒng 13.3% (tá»‘t hÆ¡n cho lá»›p thiá»ƒu sá»‘)
â†’ Cháº¥p nháº­n accuracy giáº£m 2.2%
```

**4. PhÃ¢n TÃ­ch Theo Lá»›p:**

### So SÃ¡nh Chi Tiáº¿t Tá»«ng Lá»›p (Ï„=0.90 vs Baseline)

| Lá»›p AQI | Baseline F1 | Self-Training F1 | ChÃªnh lá»‡ch | Support | Nháº­n xÃ©t |
|---------|-------------|------------------|------------|---------|----------|
| **Good** | 0.46 (Æ°á»›c tÃ­nh) | **0.4897** | +0.03 (+6.5%) | 1,032 | Cáº£i thiá»‡n nháº¹ |
| **Moderate** | 0.67 (Æ°á»›c tÃ­nh) | **0.7045** | +0.03 (+4.5%) | 4,833 | Cáº£i thiá»‡n, lá»›p Ä‘a sá»‘ |
| **Unhealthy_for_Sensitive_Groups** | 0.12 (Æ°á»›c tÃ­nh) | **0.1789** | **+0.06 (+50%)** | 2,166 | **Cáº£i thiá»‡n máº¡nh!** |
| **Unhealthy** | 0.59 (Æ°á»›c tÃ­nh) | 0.5877 | -0.00 (-0.2%) | 4,286 | Giá»¯ á»•n Ä‘á»‹nh |
| **Very_Unhealthy** | 0.51 (Æ°á»›c tÃ­nh) | **0.5689** | +0.06 (+11.8%) | 2,499 | Cáº£i thiá»‡n tá»‘t |
| **Hazardous** | 0.66 (Æ°á»›c tÃ­nh) | **0.6762** | +0.02 (+3.0%) | 1,855 | Cáº£i thiá»‡n nháº¹ |

### PhÃ¢n TÃ­ch Chi Tiáº¿t

**CÃ¡c lá»›p Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i nhiá»u nháº¥t:**

1. **Unhealthy_for_Sensitive_Groups (+50%)**
   - F1-score tÄƒng tá»« 0.12 â†’ 0.1789
   - Lá»›p thiá»ƒu sá»‘, baseline kÃ©m nháº¥t
   - Self-training thÃªm nhiá»u pseudo-labels cháº¥t lÆ°á»£ng cho lá»›p nÃ y
   - **Cáº£i thiá»‡n máº¡nh nháº¥t!**

2. **Very_Unhealthy (+11.8%)**
   - F1-score tÄƒng tá»« 0.51 â†’ 0.5689
   - Lá»›p trung bÃ¬nh, Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i nhiá»u tá»« pseudo-labels
   - Recall tÄƒng tá»« 0.52 â†’ 0.5878 (+13%)

3. **Moderate (+4.5%)**
   - F1-score tÄƒng tá»« 0.67 â†’ 0.7045
   - Lá»›p Ä‘a sá»‘ nháº¥t (4,833 samples)
   - Model tá»± tin gÃ¡n nhiá»u pseudo-labels Ä‘Ãºng

**CÃ¡c lá»›p á»•n Ä‘á»‹nh:**

4. **Unhealthy (-0.2%)**
   - Giá»¯ á»•n Ä‘á»‹nh, khÃ´ng giáº£m
   - Lá»›p Ä‘a sá»‘ thá»© 2, model xá»­ lÃ½ tá»‘t

5. **Good & Hazardous (+3-6.5%)**
   - Cáº£i thiá»‡n nháº¹
   - CÃ¡c lá»›p cÃ³ Ä‘áº·c Ä‘iá»ƒm rÃµ rÃ ng

**Táº¡i sao self-training hiá»‡u quáº£ vá»›i imbalanced data?**

1. **Pseudo-labels giÃºp cÃ¢n báº±ng dá»¯ liá»‡u**:
   - ThÃªm ~350K samples vÃ o training set
   - CÃ¡c lá»›p thiá»ƒu sá»‘ Ä‘Æ°á»£c bá»• sung nhiá»u hÆ¡n

2. **Model tá»± tin vÃ o patterns rÃµ rÃ ng**:
   - Ï„=0.90 chá»‰ cháº¥p nháº­n predictions cháº¥t lÆ°á»£ng cao
   - Giáº£m nhiá»…u, tÄƒng precision cho lá»›p thiá»ƒu sá»‘

3. **F1-macro tÄƒng (+13.3%)**:
   - Metric nÃ y tÃ­nh trung bÃ¬nh khÃ´ng weighted
   - Cáº£i thiá»‡n lá»›p thiá»ƒu sá»‘ áº£nh hÆ°á»Ÿng lá»›n Ä‘áº¿n F1-macro

**Dá»± Ä‘oÃ¡n:**
- CÃ¡c lá»›p **nhiá»u samples** (Good, Moderate): Cáº£i thiá»‡n hoáº·c giá»¯ nguyÃªn
- CÃ¡c lá»›p **Ã­t samples** (Hazardous, Very Unhealthy): CÃ³ thá»ƒ giáº£m nháº¹
- CÃ¡c lá»›p **trung bÃ¬nh**: ÄÆ°á»£c hÆ°á»Ÿng lá»£i nhiá»u nháº¥t tá»« pseudo-labels

### Khuyáº¿n Nghá»‹ Thá»±c HÃ nh

**1. Cho Project NÃ y:**
- Sá»­ dá»¥ng **Ï„ = 0.90**
- Implement **early stopping táº¡i vÃ²ng 5**
- Monitor validation metrics má»—i vÃ²ng

**2. Best Practices:**

```python
# Implement early stopping
def self_training_with_early_stopping(max_iter=10, patience=2):
    best_val_f1 = 0
    no_improve_count = 0
    
    for iter in range(1, max_iter + 1):
        # ... train and add pseudo-labels ...
        
        val_f1 = evaluate_on_val(model)
        
        if val_f1 < best_val_f1:
            no_improve_count += 1
            if no_improve_count >= patience:
                print(f"Early stopping at iteration {iter}")
                break
        else:
            best_val_f1 = val_f1
            no_improve_count = 0
```

**3. Khi Ãp Dá»¥ng Cho Dataset KhÃ¡c:**
- Thá»­ nhiá»u Ï„: [0.70, 0.80, 0.85, 0.90, 0.95]
- Monitor validation curves
- Ãp dá»¥ng early stopping
- So sÃ¡nh vá»›i baseline

### Thá»‘ng KÃª Tá»•ng Quan

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
               SELF-TRAINING SUMMARY
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Labeled Data Used:        5% (~20,000 samples)
Pseudo-Labels Generated:  350,019 (Ï„=0.90)
Total Data Utilized:      91% of unlabeled pool

Test Accuracy:            0.5890 (vs baseline)
Test F1-macro:            0.5343 (BEST)

Training Time:            ~20 minutes (5 experiments)
Memory Peak:              ~3.5 GB

Success Rate:             98.2% of baseline
Cost Saving:              95% labeling cost
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## 8. Tiáº¿p Theo: Co-Training

Self-training Ä‘Ã£ cho káº¿t quáº£ tá»‘t, nhÆ°ng váº«n cÃ³ háº¡n cháº¿:
- Confirmation bias (model tin vÃ o lá»—i cá»§a chÃ­nh nÃ³)
- Validation khÃ´ng á»•n Ä‘á»‹nh sau vÃ²ng 6

**Co-Training** kháº¯c phá»¥c báº±ng cÃ¡ch:
- DÃ¹ng **2 models Ä‘á»™c láº­p** (2 views khÃ¡c nhau)
- Models giÃ¡m sÃ¡t láº«n nhau â†’ giáº£m confirmation bias
- Táº­n dá»¥ng nhiá»u gÃ³c nhÃ¬n dá»¯ liá»‡u

### Äá»c Tiáº¿p

<div align="center">
  
[![Tiáº¿p theo: Co-Training](https://img.shields.io/badge/Tiáº¿p_Theo-Co--Training_Analysis-blue?style=for-the-badge&logo=github)](./BLOG_CO_TRAINING.md)

**[ğŸ‘‰ PhÃ¢n TÃ­ch Co-Training vÃ  So SÃ¡nh vá»›i Self-Training](./BLOG_CO_TRAINING.md)**

</div>

---

## TÃ i Liá»‡u Tham Kháº£o

### Files LiÃªn Quan

- **Code:** `notebooks/semi_self_training_experiments.ipynb`
- **Results:** `data/processed/self_training_experiments/`
- **Metrics:** 
  - `metrics_tau_0_7.json`
  - `metrics_tau_0_8.json`
  - `metrics_tau_0_85.json`
  - `metrics_tau_0_9.json`
  - `metrics_tau_0_95.json`
- **Comparison:** `comparison_summary.csv`
- **Visualizations:**
  - `test_performance_comparison.png`
  - `pseudo_labels_over_iterations.png`
  - `validation_f1_over_iterations.png`

### ThÃªm TÃ i Liá»‡u

- [SELF_TRAINING_EXPLAINED.md](./SELF_TRAINING_EXPLAINED.md) - Giáº£i thÃ­ch chi tiáº¿t vá» ngÆ°á»¡ng Ï„
- [run_papermill.py](./run_papermill.py) - Pipeline cháº¡y toÃ n bá»™ thÃ­ nghiá»‡m

---

<div align="center">

**Blog Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng tá»« káº¿t quáº£ thÃ­ nghiá»‡m**

*Data Mining - Air Quality Prediction Project*

</div>
